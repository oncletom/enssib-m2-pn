<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<!--[if IE]><meta http-equiv="X-UA-Compatible" content="IE=edge"><![endif]-->
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Asciidoctor 1.5.8">
<meta name="author" content="Benoit Epron, Marcello Vitali-Rosati">
<title>L&#8217;édition à l&#8217;ère numérique</title>
<link rel="stylesheet" href="./style.css">
</head>
<body class="article">
<div id="header">
<h1>L&#8217;édition à l&#8217;ère numérique</h1>
<div class="details">
<span id="author" class="author">Benoit Epron, Marcello Vitali-Rosati</span><br>
<span id="revnumber">version v1,</span>
<span id="revdate">2018-05-31</span>
</div>
</div>
<div id="content">
<div id="preamble">
<div class="sectionbody">
<!-- toc disabled -->
</div>
</div>
<div class="sect1">
<h2 id="_introduction">1. Introduction</h2>
<div class="sectionbody">
<div class="paragraph">
<p><strong>T</strong>extes, images, vidéos, données sont produits et circulent désormais en environnement numérique. Même les contenus destinés à l’imprimé sont dans leur totalité rédigés, structurés et mis en forme avec des outils numériques et sont ensuite commercialisés, rendus visibles et accessibles via des plates-formes en ligne.</p>
</div>
<div class="paragraph">
<p>En ce sens, il n’y a plus aucun contenu qui ne soit pas touché par les technologies informatiques.
C’est pour cette raison que l’objet de ce livre n’est pas principalement l’édition numérique, mais plutôt l’édition à l’ère numérique : son ambition est de donner un aperçu de l’impact non seulement des outils, mais plus largement de la culture numérique sur l’édition.</p>
</div>
<div class="paragraph">
<p>Dans cet ouvrage, le lecteur trouvera un survol des principaux aspects de l’édition et une analyse de la façon dont ces aspects sont en train d’être modifiés par les changements culturels, sociaux et économiques qui caractérisent notre époque.</p>
</div>
<div class="paragraph">
<p>Au cours des dernières années, la notion de « désintermédiation » a souvent été convoquée : le Web et les technologies numériques réduiraient les médiations entre la production et la publication des contenus. L’édition aurait donc perdu son importance, car n’importe quel usager pourrait, sans médiation, rendre disponibles les contenus qu’il souhaite. Au contraire, ce livre montre que « la fonction éditoriale n’a jamais été aussi présente et aussi centrale qu’aujourd’hui. Il y a de l’édition partout : dans les différents médias en ligne, mais aussi dans la structuration des contenus sur les réseaux sociaux, dans les plates-formes de distribution, dans les blogs, dans les moteurs de recherche…</p>
</div>
<div class="paragraph">
<p>Pour démontrer cela, ce livre s’articule en quatre chapitres. Le chapitre I propose une définition de l’édition et une analyse des enjeux généraux liés à la culture numérique : le changement du métier d’éditeur, la question des droits d’auteur et l’émergence du concept d’éditorialisation. Ce chapitre montre que l’édition a principalement trois fonctions : celle de produire des contenus, celle de les légitimer et celle de les faire circuler. Chacun des autres trois chapitres analyse une de ces trois fonctions. Le chapitre II analyse donc la production des contenus et montre comment l’environnement numérique détermine de nouveaux modes d’écriture et de structuration des contenus. Le chapitre III prend en compte le rôle de légitimation des contenus typique de l’édition : à la reconnaissance symbolique liée à la publication imprimée s’ajoutent de nouveaux modèles de légitimation, en particulier celui fondé sur les recommandations de la communauté et celui fondé sur celles des algorithmes. Le chapitre IV explique comment les nouveaux modes de diffusion des contenus — et en particulier le Web — ont changé les circuits et les dispositifs de circulation prénumérique en affectant le rôle des librairies et des bibliothèques.</p>
</div>
<div class="paragraph">
<p>L’édition change donc, mais elle reste fondamentale : cet ouvrage se veut une réflexion destinée à tous les lecteurs qui souhaitent mieux comprendre comment l’ensemble des connaissances, du savoir et des contenus en général sont produits, validés et diffusés à notre époque.</p>
</div>
<div class="sect2">
<h3 id="_édition_et_droits_dauteur">1.1. Édition et droits d’auteur</h3>
<div class="sect3">
<h4 id="_naissance_des_droits_dauteur_et_naissance_de_lédition">1.1.1. Naissance des droits d’auteur et naissance de l’édition</h4>
<div class="paragraph">
<p>Les analyses de l’impact du numérique dans les différents secteurs de l’édition nous
ont fait remarquer l’importance du concept d’auteur et de son statut dans l’édition. Or ce
statut ne s’est stabilisé qu’au terme d’une longue histoire, au cours de laquelle la question
des droits a joué un rôle fondamental.<br></p>
</div>
<div class="paragraph">
<p>« Le statut d’Anne », également connu comme « Copyright Act de 1710 », est un
texte   adopté   par   le   parlement   britannique   et   qui   est   habituellement   considéré   comme
l’acte   de   naissance   du   droit   d’auteur
:   il   s’agit   là,   du   moins,   de   la   première   loi   qui
réglemente   explicitement   la   publication   et   la   republication   de   livres   en   limitant   la
possibilité   de   copier   et   de   diffuser   sans   l’autorisation   de   l’auteur.   Il  est   intéressant   de
constater que cette loi apparaît à une période clé de l’institutionnalisation de l’édition,
processus auquel elle contribua fortement. Les droits d’auteur sont en effet un pilier de
l’édition   imprimée,   car   ils   structurent   la   condition   même   d’existence   des   maisons
d’édition : sans droit d’auteur, le modèle économique ayant permis à ces maisons de se
développer   et   d’avoir   une   place   si   centrale   dans   la   production   et   la   circulation   des
contenus n’existerait pas.<br></p>
</div>
<div class="paragraph">
<p>L’invention  de l’imprimerie  a créé  une certaine  séparation  entre le  contenu et le
support   sur   lequel   il   est   publié   –   une   séparation   auparavant   moins   évidente.   La
mécanisation de la copie rendue possible par la presse à caractères mobiles permettait en
effet   d’imprimer   un   même   contenu   pratiquement   sans   effort   et   à   un   coût   réduit.   Au
contraire, le manuscrit mettait sur le même plan la valeur de l’objet et la valeur de son
contenu
: les deux étaient difficilement dissociables. Dans le cas d’un livre imprimé, il est
devenu plus aisé de distinguer le contenu – reproductible  – de l’objet  – unique. De là
l’effort théorique pour inventer la notion de droit d’auteur
: l’auteur n’est pas propriétaire
de l’objet livre – qui peut donc être vendu –, mais de son contenu – ou pour être plus
précis, d’une certaine  manière  de formuler des idées qui s’inscrit ensuite dans le texte
imprimé [Rose, 1993]. L’idée contemporaine de droit d’auteur est donc née en réponse à
un problème économique et social bien précis, lié surtout à une technologie singulière de
production et diffusion des textes
: l’imprimerie.<br>
Souligner   la   contemporanéité   de   l’émergence   des   lois   sur   les   droits   d’auteur   et
l’institutionnalisation de l’édition papier implique de reconsidérer ce modèle juridique à
une époque où émergent de nouvelles  technologies de production et de circulation des
contenus. Le numérique fait naître des problèmes et des enjeux économiques et sociaux
très différents, c’est pourquoi il est nécessaire d’identifier ses caractéristiques spécifiques
pour comprendre comment notre idée des droits d’auteurs est appelée à changer.</p>
</div>
</div>
<div class="sect3">
<h4 id="_formes_de_droit_dauteur_états_uniseurope">1.1.2. Formes de droit d’auteur – États-Unis/Europe</h4>
<div class="paragraph">
<p>Avant d’identifier les défis posés par le numérique en termes de droit d’auteur, il
est nécessaire de spécifier les deux différentes interprétations auxquelles cette notion a pu
donner lieu, en Europe d’abord, puis aux États-Unis.</p>
</div>
<div class="paragraph">
<p>En   général,   s’agissant   de   l’édition   de   textes,   le   droit   d’auteur   se   fonde   sur   trois
principes [Rose, 1993] :</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>la propriété : l’auteur est propriétaire du contenu de l’œuvre qu’il a produite
;</p>
</li>
<li>
<p>la responsabilité
: l’auteur est responsable, moralement et légalement, du contenu de
son œuvre
;</p>
</li>
<li>
<p>la   singularité
:   le   contenu   de   l’œuvre   représente   la   singularité   et   l’originalité   de
l’auteur.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>On comprend que la propriété n’est possible que si on identifie une singularité du
contenu qui dépend justement de l’auteur. N’importe qui pourrait écrire une histoire du
droit   d’auteur,   alors   pourquoi   le   texte   que   vous   êtes   en   train   de   lire   appartient-il   aux
auteurs de ce livre
? Parce qu’elle représente la singularité de ces auteurs – ce qui en fait
quelque chose d’unique. Il est aussi évident que, pour en revendiquer la propriété, il est
nécessaire  d’accepter  la responsabilité  des  contenus
:  l’auteur  répond de ce  qu’il écrit.
Cette   responsabilité   –   comme   on   le   verra   dans   le   chapitre
IV
  –   implique   aussi   la
légitimation du contenu
: l’auteur garantit la fiabilité de ce qu’il écrit.<br>
Or la différence fondamentale entre l’interprétation européenne et l’interprétation
américaine du droit d’auteur est que la première associe au droit patrimonial (le fait que
l’auteur   est   propriétaire   du   contenu)   un   droit   moral   (le   respect   de   la   singularité   de
l’auteur), tandis que la seconde se concentre sur le droit patrimonial, sans mentionner le
droit moral. Dans la législation européenne, le droit patrimonial est limité dans le temps –
dans la plupart des cas 70 ans après la mort de l’auteur –, tandis que le droit moral est
pérenne.   Ce dernier   impose  le  respect  de  l’auteur  et   de son œuvre  au-delà  des   enjeux
économiques.   La   législation   européenne   souligne,   avec   cette   distinction   entre   droit
patrimonial   et   droit   moral,   la   différence   entre   les   trois   principes   du  droit   d’auteur
:   la
responsabilité et la singularité de l’auteur pouvant être pensées séparément par rapport à
la propriété, elles peuvent perdurer après la fin des droits matériels.<br>
Cette différence n’est pas sans importance
: elle rend notamment problématique la
gestion  des   droits   d’auteurs   au niveau   international.   Le  principe  du  droit  moral  est  en
effet   repris   par   la   Convention   de   Berne   pour   la   protection   des   œuvres   littéraires   et
artistiques   (1886),   accord   dont   la   fonction   est   de   régler   le   droit   d’auteur   au   niveau
international.   La   convention   stipule   que   dans   chaque   État   signataire   les   ouvrages
étrangers   bénéficient   de   la   même   protection   que   les   œuvres   locales.
Par   exemple
:   un
ouvrage d’un auteur français sera dans le domaine public au Canada cinquante ans après
la  mort  de l’auteur,  selon les  règles  canadiennes,  même  si en  France  le  droit d’auteur
s’étend à soixante-dix ans après la mort de l’auteur. Inversement, un ouvrage canadien
sera dans le domaine public en France soixante-dix ans après la mort de l’auteur, même si
au Québec la loi vaut seulement pour cinquante ans.</p>
</div>
</div>
<div class="sect3">
<h4 id="_problèmes_actuels">1.1.3. Problèmes actuels</h4>
<div class="paragraph">
<p>Les considérations sur la Convention de Berne permettent de comprendre comment
les   technologies   numériques   peuvent   mettre   en   crise   les   lois   sur   le   droit   d’auteur.   La
convention s’appuyait sur l’idée que les livres circulent physiquement dans un État ou
dans   l’autre.   L’accessibilité   d’un   livre   dans   un   État   n’impliquait   pas   son   accessibilité
dans l’autre. Le fait qu’un livre soit tombé dans le domaine public au Canada, mais pas
en France  ne posait donc que peu de problèmes
: les Français  n’auraient  pu accéder  à
l’ouvrage   qu’en   allant   physiquement   au   Canada.   Or,   dans   le   cadre   de   la   circulation
numérique, un problème se pose
: si un livre est mis à disposition gratuitement sur un
serveur au Québec, qu’empêche un Français de le télécharger, même s’il est encore sous
droit d’auteur en France
? Cet exemple montre le premier défi posé au droit d’auteur à
l’époque du numérique
:  la déterritorialisation des contenus. Un contenu sur le Web est
en   effet   potentiellement   accessible   partout   dans   le   monde.   S’il   est   vrai   que   l’on   peut
limiter cette accessibilité – notamment en bloquant les adresses IP étrangères – il est aussi
vrai que la structure même d’Internet – distribuée, non hiérarchisée – rend très difficile ce
type de contrôle et très facile son contournement. Le premier défi posé au droit d’auteur
par le numérique est donc la reconfiguration des frontières de circulation des contenus.<br>
Le deuxième défi est lié au coût de reproduction – pratiquement inexistant – des
objets   numériques
:   un   fichier   peut   être   multiplié   sans   aucun   coût   réel.   L’édition
imprimée avait déjà posé en son temps le problème de la reproductibilité
: c’est justement
de cette possibilité de reproduction à un coût assez bas qu’était née l’idée moderne de
droit d’auteur. Mais que se passe-t-il quand le support ne nécessite plus aucun coût de
production  ou de reproduction
?  Si un lecteur  est prêt  à payer  pour posséder un livre,
c’est notamment parce qu’il reconnaît sa valeur objective en tant qu’objet. Lorsque l’on
parle d’un fichier, le lecteur sait que l’objet en tant que tel n’a presque aucune valeur.<br>
Le   troisième   défi   est   déterminé   par   la   surabondance   de   contenus   sur   le   Web.
L’édition papier est basée sur une économie de la rareté
: l’information est rare et donc
précieuse.   Le   web   met   en   place   une   économie   de   l’abondance
:   les   contenus,   les
informations qu’on y trouve sont innombrables. Si dans le cadre d’une économie de la
rareté on peut être prêt à payer un auteur pour produire de nouveaux contenus, cela a-t-il
encore du sens dans notre culture numérique, où le lecteur, plutôt qu’on lui en propose
sans cesse de nouveaux, a davantage besoin qu’on l’aide à se repérer parmi les contenus
existants
?</p>
</div>
</div>
<div class="sect3">
<h4 id="_plusieurs_modèles_du_drm_aux_creative_commons">1.1.4. Plusieurs modèles : du DRM aux creative commons</h4>
<div class="paragraph">
<p>Le développement  de l’édition  numérique  a été accompagné  d’une une réflexion
importante   sur   les   outils   juridiques   appropriés.   Il   s’agit   de   répondre   à   deux   types   de
besoins a
priori
 contradictoires
: d’une part, le souhait de contrôler les usages des fichiers
numériques contenant les œuvres et notamment de lutter contre le piratage, et d’autre part
la volonté d’étendre la diffusion des œuvres en redonnant du pouvoir à leurs auteurs.<br>
Pour les éditeurs  la problématique  du piratage  était  d’autant  plus  importante  que
leur   passage   au   numérique   eut   lieu   à   une   époque   où   le   secteur   de   la   musique   était
confronté à une explosion des téléchargements illégaux. Cette période a donné lieu à la
création,   entre   autres,   de   solutions   techniques   de   contrôle   des   usages,   communément
regroupées   sous   le   terme   de   DRM   (Digital   rights   management)   ou   MTP   (Mesures
techniques   de   protection).   Les   DRM   sont   des   logiciels   qui,   associés   aux   fichiers   des
œuvres,   permettent   aux   éditeurs   de   définir   précisément   les   usages   qu’ils   souhaitent
interdire   ou   autoriser.   Cela   peut   concerner   la   possibilité   d’imprimer,   de   copier   ou
d’accéder aux fichiers. Un autre outil est le
watermarking
  (filigranage) des fichiers qui,
s’il ne vise pas une limitation des usages, permet une traçabilité des fichiers (un usager
sera moins enclin à diffuser illégalement un livre numérique sur lequel figure son nom).<br>
Dans   le   même   temps,   de   nouvelles   propositions   juridiques   ont   émergé   pour,   au
contraire,   faciliter   la   diffusion,   le   réemploi   et   le   partage   de   contenus.   Parmi   ces
propositions, la plus visible est celle des licences
creative commons,
  lancées en 2001 à
l’initiative du juriste américain Lawrence Lessig, qui permettent à un auteur de définir
a
priori
  les   conditions   d’usages   et   éventuellement   de   réutilisation   de   ses   œuvres.   Ces
licences   précisent   les   modalités   de   mention   de   l’auteur   et   les   possibilités   d’utilisation
commerciale   et   de   modification   d’un   contenu.   Une   licence   CC0   permet   même   de
renoncer à ses droits, dans la limite du cadre législatif de chaque pays.<br></p>
</div>
<div class="paragraph">
<p>+</p>
</div>
<div class="paragraph">
<p>+</p>
</div>
<div class="paragraph">
<p>+</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 25%;">
<col style="width: 25%;">
<col style="width: 25%;">
<col style="width: 25%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Schéma de
DRM</th>
<th class="tableblock halign-left valign-top">Propriétaire</th>
<th class="tableblock halign-left valign-top">Plate-forme de
distribution</th>
<th class="tableblock halign-left valign-top">Particularités</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">DRM
d’Amazon</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Amazon</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Amazon (Kindle)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Interdiction de copier ou de
transférer un e-book dans une autre
bibliothèque. Certains e-books
peuvent être prêtés à un autre
utilisateur pour une durée de 14
jours.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Adobe
Digital
Experience
Protection
Technolog
y (ADEPT)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Adobe</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Adobe Digital
Editions</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Visionnement des contenus sur un
maximum de six appareils.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">FairPlay</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Apple</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">iBooks Store</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Les contenus achetés ne peuvent être
lus que sur les liseuses d’Apple.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Marlin</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Marlin
Developer
Community
(MDC)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Kno</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Marlin Developer Community est
composé de cinq sociétés
: Intertrust,
Panasonic, Philips, Samsung et
Sony. La vente de la licence est
assurée par la Marlin trust
management organization (MTMO)</p></td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_ii_la_production_des_contenus">2. II / La production des contenus</h2>
<div class="sectionbody">

</div>
</div>
<div class="sect1">
<h2 id="_la_plate_forme_et_lécriture">3. La plate-forme et l’écriture</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_les_techniques_et_la_pensée">3.1. Les techniques et la pensée</h3>
<div class="paragraph lead">
<p>Les outils façonnent la pensée. Ce que nous pouvons penser et ce que nous pouvons dire résulte d’une dynamique dans laquelle les outils et les techniques jouent un rôle fondamental. L’idée spiritualiste selon laquelle la pensée se construit au-delà de toute matérialité et que les outils et les techniques ne servent qu’à la faire apparaître a été profondément critiquée, en particulier au XXe siècle : pensons par exemple à la célèbre phrase de McLuhan « le médium est le message » ou aux analyses de Jack Goody [1979] sur le rapport étroit entre l’écriture et la naissance de la pensée scientifique [Deseilligny, 2013].</p>
</div>
<div class="paragraph">
<p>Ce qui nous intéresse plus particulièrement ici est le fait que les outils et les techniques d’écriture ne sont pas neutres par rapport aux contenus que l’on peut produire. Pour être plus précis : les techniques et les outils d’écriture, mais aussi les modes de diffusion des textes, conditionnent et façonnent ce que l’on écrit.</p>
</div>
<div class="paragraph">
<p>Du côté des modes de diffusion, on peut ainsi souligner la différence fondamentale entre la forme orale et la forme écrite. Comme le remarque Christian Vandendorpe, les contenus transmis dans les traditions orales sont caractérisés par un aspect formulaire – c’est-à-dire ponctué de formules récurrentes telles que l’on peut en observer dans les poèmes homériques, par exemple – qui limite de fait l’éventail des potentialités des récits, lesquels ont tendance à se standardiser. « Les cultures orales ne s’expriment pas seulement en formules, elles pensent en formules » [Ong, 1977 cité par Vandendorpe, 1999, p. 26]. L’écriture permet l’émergence de contenus à l’organisation plus complexe, ainsi que leur conservation et leur consultation. Pour autant, le papyrus – support linéaire qui ne peut être consulté que dans un ordre prédéfini – n’offre pas encore les possibilités du codex, support divisé en pages, plus maniable, qui permet de passer facilement d’un point du texte à un autre. Cette distinction a des conséquences majeures : aurait-on pu penser une encyclopédie sur papyrus ? Le concept d’encyclopédie serait-il apparu sans l’invention du codex ?</p>
</div>
<div class="paragraph">
<p>Quant aux techniques et aux outils à proprement parler, on connaît déjà l’influence des instruments sur le statut de l’écriture : de la plume – réservée à une élite – à la démocratisation du stylo à bille, un changement radical de la valeur symbolique de l’écriture s’est opéré. Entre l’imagerie de l’auteur à sa table de travail, plume à la main, et l’omniprésence industrielle du stylo à bille qui, comme l’affirme Umberto Eco, est probablement le « seul exemple de socialisme réalisé », on s’aperçoit qu’écrire n’a plus le même sens. Mais outre la dimension symbolique, il est évident que la vitesse de l’écriture, tout comme la valeur économique du papier et son format, ou encore les possibilités d’effacer ce qui a été écrit, changent non seulement les pratiques d’écriture, mais aussi le contenu même de ce que l’on peut écrire.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_les_outils_numériques">4. Les outils numériques</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Les outils d’écriture et de diffusion n’ont cessé d’évoluer dans l’histoire de nos sociétés. Les changements survenus à la fin du XXe siècle et au début du XXIe sont cependant particulièrement impressionnants. On peut résumer ces changements en cinq grandes catégories : 1) la maniabilité du texte ; 2) le rapprochement entre texte manuscrit et texte diffusé ; 3) la multiplication des formats ; 4) les possibilités de structuration ; 5) les possibilités de recherche de ressources pour l’écriture.</p>
</div>
<div class="paragraph">
<p>Tout d’abord (point 1), l’introduction des logiciels de traitement de texte, dont la diffusion a été généralisée à partir des années 1980 [Kirschenbaum, 2016], a profondément changé les possibilités de manipulation du texte. À la différence de l’écriture au stylo ou à la machine à écrire, le texte numérique peut être facilement effacé, copié, collé, restructuré. Cela implique notamment (point 2) le brouillage de la frontière entre le brouillon et la version définitive du texte, ainsi que la disparition progressive de figures professionnelles comme le dactylographe. Entre l’écriture et la diffusion des contenus, le processus éditorial compte ainsi une médiation en moins. La forme même du texte – à partir des logiciels WYSIWYG (accronyme de l’anglais « what you see is what you get », « ce que vous voyez est ce que vous obtenez » parfois traduit par « tel-tel » en français) – se rapproche sensiblement de la forme finale du texte imprimé. Ces deux premiers changements œuvrent en faveur d’une plus forte accessibilité à la publication pour tous. Ils nourrissent par ailleurs un sentiment de marginalisation des processus de médiation entre le manuscrit et le texte publié.</p>
</div>
<div class="paragraph">
<p>Mais il ne faut pas faire l’erreur de considérer les outils numériques comme un tout homogène (point 3) : s’il est vrai que certains logiciels de traitement de texte – et en particulier Microsoft Word – se sont imposés dans les pratiques, il est aussi vrai que les logiciels et les formats n’ont cessé de se diversifier. Il est important de souligner que chaque format et chaque logiciel porte une idée particulière de ce que signifie « écrire » et « publier ». Les différents types de visualisation du texte influent sur la perception de l’acte d’écrire, mais surtout, la structuration même du matériel écrit change selon les formats. En particulier, il y a une différence fondamentale entre les formats orientés vers la mise en forme graphique du texte et ceux orientés vers la structuration sémantique (point 4). Les premiers requièrent un balisage des attributs graphiques du texte – choix de la police et de la taille du caractère, usage de l’italique ou du gras, disposition du texte sur la page, etc. Typiquement, Word – et tous les autres logiciels WYSYWYG – est fondé sur ce principe. Les formats RTF (Rich text format) et doc (ou docx) sont représentatifs de cette approche. Le sens du texte est exprimé à travers sa mise en forme graphique. Ce type d’approche tend à considérer le texte numérique lors de sa production comme une étape avant l’impression.</p>
</div>
<div class="paragraph">
<p>Un autre approche consiste à baliser le texte selon sa valeur sémantique : c’est le cas des outils basés sur les technologies XML (Extendible markup language), où l’on indique par exemple qu’une partie du texte est un titre de niveau 1 ou 2, une note, ou, de manière plus précise, qu’un mot est le nom d’une ville ou d’une personne, etc. Les formats sémantiques ont la caractéristique de permettre une forte structuration logique du texte, ce qui était impossible sur papier. En ce sens, ces formats représentent un changement important par rapport au modèle de l’imprimé. Lors de l’écriture, il devient important de baliser ce que l’on écrit par rapport à son sens, afin d’être ensuite capable de manipuler automatiquement le texte.</p>
</div>
<div class="paragraph">
<p>Pour finir, écrire en environnement numérique signifie – au moins depuis les années 1990 – avoir accès, sur le même support qu’on utilise pour écrire (ordinateur, tablette…), à un grand nombre de sources potentiellement mobilisables (point 5) : le Web propose en effet une quantité impressionnante de contenus qui peuvent servir de documentation. Tous les types d’écriture sont concernés : de la littérature – pensons aux accusations de plagiat de Wikipédia adressées à Michel Houellebecq pour son roman La Carte et le Territoire – aux textes scientifiques – lors de l’écriture desquels le chercheur dispose d’un accès numérique à un grand nombre de sources.</p>
</div>
<div class="paragraph">
<p>Une réflexion s’impose quant à la façon dont le numérique modifie la phase de production des contenus éditoriaux, et aux enjeux intellectuels et politiques qui en découlent. Nous n’avons pas fait qu’inventer des outils permettant de réaliser plus rapidement et plus efficacement des tâches déjà connues : nous faisons désormais face à un environnement d’écriture qui bouleverse profondément ce que signifie écrire.</p>
</div>
<div class="paragraph">
<p>Parallèlement, il faut insister sur le fait que les outils numériques ne peuvent pas être considérés comme un tout. Il ne s’agit donc pas de chercher comment « le numérique » en général change notre façon de produire des contenus, mais plutôt de réfléchir aux différentes formes d’écriture que les outils numériques proposent, pour comprendre comment les choix technologiques reflètent nos conceptions de l’écriture. En d’autres termes, si l’on ne saurait dire ce que fait exactement le numérique à l’écriture – et en général à la pensée – il faut du moins analyser les visions du monde et de l’écriture au fondement de chaque outil.</p>
</div>
<div class="paragraph">
<p>Tableau 2</p>
</div>
<div class="imageblock">
<div class="content">
<img src="tab_2.png" alt="Principaux formats de textes associés aux logiciels qui permettent de les traiter.">
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_iii_la_légitimation_des_contenus">5. III / La légitimation des contenus</h2>
<div class="sectionbody">
<!-- toc disabled -->
<div class="sect2 text-justify">
<h3 id="_les_modèles_classiques">5.1. Les modèles « classiques »</h3>
<div class="sect3">
<h4 id="_la_reconnaissance_symbolique_de_lédition_papier">5.1.1. La reconnaissance symbolique de l’édition papier</h4>
<div class="videoblock">
<div class="content">
<iframe src="https://www.youtube.com/embed/AcF9ORS-rho?rel=0&amp;hl=fr" frameborder="0" allowfullscreen></iframe>
</div>
</div>
<div class="paragraph">
<p>De quelle manière peut-on donner de la crédibilité à un contenu ? Comment faire
en sorte qu’un texte, un document, une information soient considérés comme fiables ?
Comment donner de l’autorité à des affirmations ou à des idées ? Voici quelques unes des
questions fondamentales auxquelles doit répondre aujourd’hui la fonction éditoriale.
Comme on l’a vu, les maisons d’édition comptent, au moins depuis le XVIIIe siècle,
parmi les instances qui remplissent la fonction de légitimation. La légitimité du contenu
est fonction de la légitimité symbolique de la maison d’édition qui le publie. Mais
comment une maison d’édition peut-elle concrètement être source de légitimation ? Pour
répondre à cette question, on peut identifier trois différents types de légitimation : le
prestige, la visibilité et l’évaluation scientifique.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Le prestige dépend de la réputation qu’une maison d’édition a pu acquérir auprès
des lecteurs. Ces derniers connaissent le nom de la maison et associent à ce nom une
idée de qualité. Cette valeur symbolique est transférée de la maison d’édition aux
ouvrages qu’elle publie. Concrètement, le nom et le logo de la maison confèrent du
prestige à l’ouvrage et garantissent sa qualité. Ce type de légitimation est typique
des contenus destinés au grand public.</p>
</li>
<li>
<p>La visibilité dépend de la capacité à faire connaître l’existence d’un contenu à un
certain nombre de personnes. Les maisons d’édition ont la possibilité de
communiquer sur leurs publications et de les rendre visibles. Si cette visibilité ne
garantit pas nécessairement la qualité de l’ouvrage, elle participe tout de même de sa
légitimation, car elle en détermine l’influence. Dans cette production de visibilité,
plusieurs maillons de la chaîne de valorisation du livre peuvent jouer un rôle
fondamental, comme les prix littéraires, qui augmentent l’audience des ouvrages et
se présentent comme un gage de qualité.</p>
</li>
<li>
<p>L’évaluation scientifique concerne seulement un type particulier de contenus : ceux
destinés à un public de spécialistes – notamment d’universitaires –, souvent publiés
par des maisons d’édition spécialisées – comme des presses universitaires – ou dans
des collections particulières. Chaque ouvrage est en général soumis, avant
publication, à un processus d’évaluation par les pairs : la qualité et la rigueur
scientifique du contenu sont analysés et appréciés, souvent en aveugle, par des
spécialistes. Ce processus garantit la légitimité de l’ouvrage. Dans ce cas, la maison
d’édition est le garant du bon déroulement du processus d’évaluation, les spécialistes
auxquels est demandée l’évaluation étant extérieurs à la maison.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>Un contenu peut bénéficier de ces trois types de légitimation en même temps : avoir
du prestige, de la visibilité et être garanti par une évaluation scientifique. Souvent, ces
aspects sont liés : la visibilité peut dépendre du prestige, le prestige du sérieux de
l’évaluation scientifique, etc. Mais ces types de légitimation peuvent aussi jouer de
manière isolée : on peut prendre l’exemple d’une petite maison d’édition universitaire, à
la visibilité très réduite et au prestige parfois limité, ou d’une grande maison d’édition
commerciale, se donnant les ventes pour seul objectif, et misant tout sur la visibilité.
Les environnements numériques ont indéniablement produit de nouveaux modèles
de légitimation. Ils ont ainsi, en partie, remis en question le monopole que les maisons
d’édition avaient conservé pendant plusieurs siècles en matière de légitimation. Mais il
serait faux d’affirmer que le numérique a enlevé – ou même affaibli – le pouvoir de
légitimation et la valeur symbolique des maisons d’édition. Le fait d’être édité par l’une
d’entre elles – en papier ou en format numérique – reste la source principale de légitimité
pour un contenu, même si ce n’est plus la seule.</p>
</div>
<div class="paragraph">
<p>$ asciidoctor -a toc epron.adoc</p>
</div>
<div class="imageblock text-center text-justify">
<div class="content">
<img src="epron%20image.jpg" alt="epron image">
</div>
</div>
<div class="videoblock">
<div class="content">
<iframe src="https://www.youtube.com/embed/Q7RZWgegqM0?rel=0&amp;hl=fr" frameborder="0" allowfullscreen></iframe>
</div>
</div>
<div class="paragraph">
<p>==ABC</p>
</div>
<div class="paragraph">
<p>L’auteur se porte bien
une autre forme de légitimation d’un contenu provient de son auteur – ou, très
concrètement, du nom de l’auteur affiché sur la couverture d’un livre.
On peut identifier trois éléments propres à la fonction auctoriale en partie croisés
avec ceux de la fonction éditoriale. Tel qu’elle s’est stabilisée à partir du XVIIIe siècle, la
fonction auctoriale implique la propriété, l’originalité et enfin l’autorité [Rose, 1993].</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>l’auteur a tout d’abord un certain droit de propriété sur ce qu’il produit. Cette notion
s’est affirmée et institutionnalisée à partir du XVIIIe siècle (le Statut d’Anne de 1710
est considéré, on l’a vu, comme la première loi sur le copyright). L’idée de lier
l’auctorialité à la propriété a rendu théoriquement possible la naissance de l’édition
papier telle que nous la connaissons. Le fait que l’auteur soit propriétaire du contenu
– et non du support qui le fait circuler – lui permet de vendre le droit d’exploitation
de son bien, notamment à une maison d’édition. Ce principe est à la base du modèle
économique de l’édition papier.</p>
</li>
<li>
<p>En second lieu, l’auteur exprime un point de vue particulier et original, car il
manifeste, dans ses productions, son individualité. Les notions d’individu et
d’originalité deviennent essentielles pour expliquer la propriété intellectuelle :
l’auteur est propriétaire du fruit de son originalité, à savoir de ce qu’il est le seul à
pouvoir produire. La fonction auctoriale garantit donc aussi la reconnaissance de
l’auteur, son style, sa « marque ».</p>
</li>
<li>
<p>En dernier lieu, l’auteur légitime et garantit les contenus qu’il produit. Il en est donc
aussi le responsable – y compris d’un point de vue légal. Cette fonction est
justement partagée avec l’éditeur, qui augmente et encadre la légitimité de l’auteur.
Les deux fonctions de légitimation – auctoriale et éditoriale – sont alors entremêlées.
Dans certains cas, l’une prévaut sur l’autre : par exemple, un auteur encore inconnu
verra sa légitimité garantie par la fonction éditoriale ; à l’inverse, un auteur très
connu pourra faire profiter de sa légitimité à un nouvel éditeur.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>=DEF</p>
</div>
<div class="paragraph">
<p>Qu’en est-il de ces structures dans les environnements numériques ? Depuis
quelques années, on entend souvent parler d’une crise de l’auteur. Cette crise est
indéniable, mais il serait faux de l’assimiler à une disparition de la fonction auctoriale. La
crise est due à la convergence d’une théorie critique du modèle de l’auteur que l’on vient
de décrire et de l’émergence de pratiques en environnement numérique qui mettent en
question l’unicité de ce modèle. À partir des années 1960, en effet, des théoriciens
comme Roland Barthes et Michel Foucault ont annoncé la « mort de l’auteur ». Leur
objectif était d’attaquer la notion romantique de l’auteur-créateur, du génie produisant à
partir de rien, dans l’originalité absolue. À ce modèle s’oppose une idée de la circulation
des textes et des idées qui ne relève jamais d’un individu isolé, mais d’une série
complexe de dynamiques culturelles, sociales et politiques. Ce n’est pas l’auteur qui
produit à partir de rien des contenus, ce sont les interactions sociales et culturelles qui les
font émerger. Or le Web – et en particulier le Web 2.0 – a permis l’émergence de
pratiques qui vont tout à fait dans ce sens : le nom de l’auteur s’efface au profit d’une
écriture collective, la signature de l’auteur devenant moins importante que celle de la
plate-forme. Les affirmations « j’ai lu cela sur Internet » ou « je l’ai trouvé sur Google »
sont à cet égard bien plus significatives qu’on pourrait le croire.
Or, s’il est vrai que de nouveaux modèles de légitimation sont en train d’émerger, il
n’en reste pas moins que la fonction auctoriale conserve son rôle de légitimation. Les
environnements numériques sont caractérisés par une hybridation de formes
traditionnelles et de formes classiques. Prenons l’exemple des blogs. Comme l’a
démontré Dominique Cardon [2010], un blogueur peut devoir sa réputation à la
popularité de son blog – et dans ce cas c’est le blog qui légitime l’auteur –, mais l’inverse
est aussi vrai : un blog peut être lu car le blogueur est déjà connu en tant qu’auteur. Ce
type de phénomène est très courant dans les environnements numériques, où les auteurs
peuvent démultiplier leur visibilité et donc leur autorité. S’il y a bel et bien une certaine
crise du concept d’auteur liée à l’émergence de nouveaux modèles de production et de
légitimation de contenus, l’auteur et la fonction auctoriale n’en ont pas pour autant perdu
leur propre pouvoir de légitimation.</p>
</div>
<div class="paragraph">
<p>Unresolved directive in index.adoc - include::../chap3-partie1.adoc[]</p>
</div>
<div class="paragraph">
<p>Unresolved directive in index.adoc - include::../Chap3-partie2.adoc[]</p>
</div>
<div class="sect4">
<h5 id="_tableau_6_les_algorithmes_et_moteurs_de_recherche_du_web">Tableau 6. Les algorithmes et moteurs de recherche du Web</h5>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 10%;">
<col style="width: 10%;">
<col style="width: 10%;">
<col style="width: 70%;">
</colgroup>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Nom</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Propriétaire</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Créateurs (date de création)</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Description</strong></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Yahoo! Search</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Yahoo inc.</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">(1995)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">À ses débuts, Yahoo! Search est moins un moteur de recherche
algorithmique qu’un annuaire web animé par l’ambition d’un classement
humain, manuel, du Web. À partir de 2000, Yahoo! Search est alimenté par
le moteur de recherche d’Inktomi, société rachetée par Yahoo en 2002, puis
par celui de Google jusqu’en 2004, date à laquelle Yahoo commence à
utiliser son propre algorithme. Cependant, depuis 2009, Yahoo est de
nouveau alimenté par un fournisseur externe, Microsoft Bing.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">PageRank</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Google</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Larry Page et Sergueï Brin (1996)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">L’algorithme célèbre du moteur de recherche de Google, fondé sur un
principe méritocratique de réputation des sites web. S’inspirant du Science
Citation Index d’Eugène Garfield (1964), il mesure en l’autorité d’un site
web en calculant les liens entrants comme des votes, ces votes ayant plus
de poids lorsqu’ils proviennent eux-mêmes d’un site réputé.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Hilltop Algorithm</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Google (rachat en février 2003)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Krishna Bharat et George A. Mihăilă (1999)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Développé à l’Université de Toronto en 1999, l’algorithme Hilltop cherche
à identifier les résultats les plus pertinents lorsqu’un internaute effectue une
recherche vaste (ou peu détaillée) en attribuant une valeur d’autorité aux
pages web. Le classement du Hilltop est réalisé au moyen d’un index de
« documents d’experts », qui sont des pages web identifiées comme
expertes sur un certain sujet. Plus les relations hypertextuelles entre une
page web et sa page experte associée sont proches, plus la page en question
sera qualifiée d’autoritaire (« <em>authoritative</em> »). L’algorithme Hilltop est
utilisé par Google depuis 2003.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">HITS Algoritm</p></td>
<td class="tableblock halign-left valign-top"></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Jon Kleinberg (1999)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">L’algorithme HITS (Hyperlink-induced topic search) classe les pages web
en leur attribuant deux scores définis par récursion mutuelle : 1) leur valeur
de référenciation (<em>hub value</em>), qui équivaut à la valeur de leurs liens
renvoyant vers d’autres pages ; 2) leur valeur d’autorité (<em>authority value</em>),
qui correspond à la valeur du contenu de la page, calculée par la somme des
hub values des liens entrants. L’article de Kleinberg dans lequel il esquisse
le fonctionnement du HITS constituera une source d’influence pour Page et
Brin dans la conception du PageRank.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">TrustRank</p></td>
<td class="tableblock halign-left valign-top"></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Hector Garcia-Molina, Zoltán Gyöngyi et Jan Pedersen (2004)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Exposé lors d’une conférence de la Very Large Data Bases Endowment
Inc. à Toronto, TrustRank est une technique d’analyse semi-automatique
des liens visant à distinguer les pages web pertinentes des pages employant
une stratégie de spam de mots-clés et de liens sortants. En ce sens, le
TrustRank s’inscrit dans la même lignée que le PageRank en ce qu’il utilise
le réseau des liens comme mesure de qualité des pages web, mais la
nécessité d’un contrôle humain de l’algorithme vise à combattre avec
davantage de précision certaines tactiques du Search Engine Optimization
(SEO ou « optimisation pour les moteurs de recherche », désignant les
techniques d’optimisation de l’indexation et du référencement).</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Bing</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Microsoft</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">(2009)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Bing est l’aboutissement d’une longue succession de moteurs de recherche
créés par Microsoft, à savoir MSN Search (1998), Windows Live Search
(2006) et Live Search (2007). Peu d’informations sur l’algorithme de Bing
sont rendues publiques, mais Microsoft a été accusé en 2010 de copier les
résultats de recherche de Google.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">EdgeRank</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Facebook</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Serkan Piantino (v. 2010)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Succédant à <em>Turning Knobs</em>, l’algorithme qui avait accompagné
l’introduction du <em>News Feed</em> en 2006, EdgeRank procède à une
hiérarchisation des informations selon une métrique d’affinités. Chaque
fois qu’un utilisateur interragit sur Facebook, il produit un <em>edge</em> qui reçoit
ensuite un score calculé à partir de trois variables : 1) Ue, ou la proximité
relationnelle entre l’utilisateur-acteur et l’utilisateur-lecteur (le score
d’affinité), elle-même déterminée par une série de paramètres (fréquence
de visite du profil, fréquence des conversations entretenues sur le site,
etc.) ; 2) We, qui correspond au poids de l’<em>edge</em>, c’est-à-dire à la nature
même de l’action de l’utilisateur (publication, <em>like</em>, commentaires, partage,
etc.) ; 3) De, le vieillissement (<em>decay</em>) du <em>edge</em> en fonction du temps passé
depuis sa création. En somme, le score d’un <em>edge</em> peut être calculé grâce à
la formule suivante : Σ UeWeDe ; plus le score est élevé, plus l’action prise
par l’utilisateur-acteur a des chances d’apparaître sur le fil d’actualité de
l’utilisateur-lecteur. Cependant, le terme d’EdgeRank est aujourd’hui
tombé en désuétude : devant la massification des usages, la
commercialisation croissante du réseau et la multiplication des appareils
mobiles, ces trois variables ont été submergées par près de 100 000 facteurs
de poids (types de publication, personnalisation des paramètres par
l’utilisateur, publicités, appareils utilisés, vitesse de connexion, etc.).</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD
Unresolved directive in index.adoc - include::Chapitre_1_les_éditeurs_face_au_numérique.adoc[]</p>
</div>
<div class="exampleblock">
<div class="content">
<div class="imageblock related thumb right">
<div class="content">
<img src="epron%20image.jpg" alt="benoit-epron" width="200" height="200">
</div>
</div>
<div class="quoteblock">
<blockquote>
De 2005 à 2010, le livre numérique a progressivement intégré tous les maillons de la chaîne du livre, cependant que les enjeux numériques étaient placés au premier plan de la politique menée par le ministère de la Culture et de la Communication, obligeant le Service du livre et de la lecture à s’adapter à ce contexte nouveau.
</blockquote>
<div class="attribution">
&#8212; Benoît Epron<br>
<cite>ancien professeur référent du Master Publication Numérique à l&#8217;ENSSIB</cite>
</div>
</div>
<div class="paragraph text-justify">
<p>= Les éditeurs face au numérique
:toc:
:toc-title: Table des matières</p>
</div>
<!-- toc disabled -->
<div class="paragraph text-justify">
<p>== Des enjeux très hétérogènes</p>
</div>
<div class="paragraph">
<p>&#160; La filière de l’édition présente, au-delà de l’apparente homogénéité d’un secteur d’activité, une situation hétérogène dans son rapport au numérique. La structuration des secteurs éditoriaux et des marchés n’est pas la même et induit donc des approches différentes vis-à-vis du développement de l’écosystème numérique. Ces approches diffèrent notamment en raison des différences de statut des auteurs, utilisateurs, clients ou acheteurs.</p>
</div>
<div class="paragraph">
<p>&#160; Le rapport au numérique de chaque secteur est fonction de différents paramètres, comme l’existence d’un marché professionnel (édition juridique), le degré de
concentration des acteurs (édition scientifique) ou la nature des acheteurs (édition scolaire). Ces enjeux sont mouvants, l’édition numérique s’inscrivant dans l’évolution
constante et rapide du contexte technologique et des usages.</p>
</div>
<div class="paragraph">
<p>&#160; Cette évolution présente des disparités importantes. En France, le Syndicat national de l’édition (SNE) estime la part du numérique dans le chiffre d’affaires global de
l’édition à 8,65 % en 2016. Dans le même temps, la part du numérique dans le marché étatsunien de l’édition est estimée à plus de 33 %.</p>
</div>
<div class="paragraph">
<p>&#160; À grands traits, les enjeux des différents secteurs de la filière de l’édition sont les suivants :</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>La définition de modèles économiques hybrides qui permettent de gérer la coexistence à plus ou moins long terme d’une activité éditoriale sur
supports numériques et sur supports papier. C’est notamment le cas pour l’édition de littérature grand public qui voit la part de son activité relative
au numérique croître progressivement.</p>
</li>
<li>
<p>La définition de formes éditoriales numériques pertinentes et adaptées. C’est notamment le cas pour l’édition scolaire ou l’édition jeunesse dans
lesquelles se posent des questions relatives à l’interactivité ou aux fonctionnalités embarquées. Ces questionnements portent sur l’efficience
ou la valeur ajoutée réelle de ces fonctionnalités ou enrichissements. Depuis les années 2000, le concept d’<em>édition augmentée</em> est étudié et
discuté. Ses applications sont encore limitées, mais constituent une voie intéressante pour certains secteurs.</p>
</li>
<li>
<p>L’adaptation des modèles, des filières et des acteurs. Ces enjeux politiques
ne sont pas spécifiques à certains secteurs même si l’édition scientifique en
est un bon exemple. En effet, il s’agit d’un secteur déjà quasiment
entièrement numérique, dans lequel des débats politiques posent la
question de la place des différents acteurs, publics ou privés, dans cette
activité.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>&#160; Ces enjeux couvrent également des aspects transversaux de l’édition numérique : le
statut des auteurs de livres enrichis, la place des bibliothèques dans ce nouvel
écosystèmes ou encore l’environnement législatif, concernant le prix unique du livre ou
les durées d’embargo par exemple. Ces questionnements sont complexes car ils mêlent
des aspects techniques, politiques et économiques.</p>
</div>
<div class="videoblock text-center">
<div class="content">
<iframe width="640" height="480" src="https://www.youtube.com/embed/F9OBL49LjWg?rel=0&amp;hl=fr" frameborder="0" allowfullscreen></iframe>
</div>
</div>
<div class="paragraph text-justify">
<p>== La littérature : nouvelles formes de diffusion</p>
</div>
<div class="paragraph">
<p>&#160; La littérature est, en termes de visibilité et de poids symbolique, un secteur central
dans l’analyse du passage au numérique de l’édition. La littérature représente en effet,
d’après les chiffres du SNE, environ 25 % du chiffre d’affaire de l’édition en France,
devant les secteurs de la jeunesse et du livre pratique. Ce secteur a longtemps concentré
les attentions et analyses lorsqu’était évoqué le <em>livre numérique</em>. Cette focalisation
s’explique notamment par la visibilité dont bénéficie la littérature auprès du grand public
et par le rôle matriciel qu’elle pourrait jouer pour les autres secteurs. En effet, depuis le
Village eBook de l’édition 2000 du Salon du livre de Paris, la question du livre
numérique a longtemps été abordée sous le double angle du dispositif de lecture et du
modèle économique.</p>
</div>
<div class="paragraph">
<p>&#160; Dans les années 2000, les expériences de développement de dispositifs techniques
de lecture numérique se multiplient avec notamment le lancement du CyBook par la
société Cytale en 2001. Ce dispositif, un écran d’ordinateur portable tactile intégré à une
coque recouverte d’un rabat en cuir, ouvre plusieurs questionnements. Tout d’abord, il
pose la question du matériel nécessaire à une activité de lecture numérique. Le
rétroéclairage de l’écran, ses performances d’affichage, la taille ou le poids du dispositif
deviennent des problématiques auxquelles il apparaît indispensable de répondre avant
d’imaginer le développement d’un marché du livre numérique. Les acteurs du livre, en
l’occurrence les éditeurs, se retrouvent donc confrontés à de nouveaux types
d’interlocuteurs avec d’un côté les fabricants de dispositifs techniques de lecture qui
conditionnent, de façon logicielle ou matérielle, l’expérience de lecture et, de l’autre, les
plates-formes dédiées au livre numérique, qu’elles soient de commercialisation et/ou
d’autoédition. Ce secteur de l’édition est ainsi confronté à de multiples formes
émergentes de diffusion. Comme pour l’ensemble des modèles numériques de document,
cette transformation de la diffusion se structure autour d’une tension entre
désintermédiation et réintermédiation.</p>
</div>
<div class="paragraph">
<p>&#160; D’une part, il s’agit d’une dynamique de diffusion <em>directe</em> des productions
éditoriales, de l’auteur au lecteur, dans une logique d’autoédition. Ce modèle n’est pas
propre à la littérature numérique – il existait déjà des éditions à compte d’auteur dans le
monde de l’imprimé – mais elle prend sur Internet une dimension particulièrement
importante. D’autre part, la réintermédiation, est le mécanisme d’émergence de nouveaux
acteurs intermédiaires dans la diffusion des produits éditoriaux numériques. Le point à
souligner dans ce mouvement de réintermédiation est la nature des nouveaux acteurs,
parfois nouveaux entrants dans le domaine du livre, qui déploient des modèles
économiques dans lesquels la part de leurs revenus issue de la vente des ouvrages
numériques reste très limitée. Ainsi, en réintermédiant leur diffusion numérique, les
éditeurs traditionnels se trouvent confrontés à des acteurs ayant des objectifs stratégiques
potentiellement très éloignés. Dans le cas d’Apple par exemple, la diffusion des produits
éditoriaux numériques ne représente qu’une part très réduite de son activité, au contraire
de la vente de matériel. Le cas d’Amazon est différent : en soutenant un tarif relativement
bas pour ses liseuses, c’est davantage la croissance de sa base de clients qui est
recherchée. Cette divergence en termes d’enjeux stratégiques complique d’autant plus la
relation de coopération entre les éditeurs et ces diffuseurs numériques qu’elle s’inscrit
dans un rapport de forces nettement à l’avantage des plates-formes de diffusion. Des
tentatives de reprise en main de la distribution numérique par les éditeurs existent
toutefois. Certaines, comme Immatériel.fr ou Dilicom, se positionnent comme
intermédiaire de distribution vers les plates-formes de commercialisation, tandis que
d’autres se focalisent sur des secteurs spécifiques, comme Cairn pour l’universitaire.</p>
</div>
<div class="paragraph text-justify">
<p>== L’édition jeunesse : des nouvelles productions multimédia</p>
</div>
<div class="paragraph">
<p>&#160; L’édition jeunesse est à considérer précisément dans sa confrontation au
numérique. Si l’édition d’ouvrages traditionnels destinés à la jeunesse (<em>Harry Potter</em> par
exemple) s’inscrit dans des logiques proches de celles de la littérature, le cas de l’édition
illustrée, augmentée, interactive… est bien différent. Nous appréhenderons ici
uniquement ce second cas pour mieux identifier les enjeux spécifiques auxquels ce
secteur est aujourd’hui confronté.</p>
</div>
<div class="paragraph">
<p>&#160; La particularité de l’édition jeunesse dans son passage au numérique réside dans la
spécificité des formes éditoriales qu’elle prend. Par formes éditoriales, nous entendons
l’ensemble des fonctionnalités exploitant les potentialités interactives et multimédia
propres au numérique. Dans ce domaine, les enjeux pour l’édition jeunesse se situent à
plusieurs niveaux.</p>
</div>
<div class="paragraph">
<p>&#160; Dans un premier temps, la question de la pertinence de ces fonctionnalités
nouvelles dans la création de productions éditoriales pour la jeunesse se pose. Il ne s’agit
pas d’une problématique récente : les acteurs impliqués dans les productions <em>éditoriales</em>
sur supports numériques (comme le CD par exemple) ont du traiter ces questions depuis
plusieurs décennies déjà. Ainsi, l’apport d’éléments multimédias dans les trames
narratives ou dans l’illustration de récits est une question qui s’est déjà largement posée à
la fois aux éditeurs et aux auteurs d’oeuvres destinées à la jeunesse. Elle se pose
aujourd’hui de manière différente en raison notamment du renouvellement permanent des
possibilités offertes par le numérique (comme la réalité virtuelle ou augmentée, pour ne
parler que des pistes les plus récentes).</p>
</div>
<div class="paragraph">
<p>&#160; Dans un second temps, l’évolution des formes éditoriales induit une
complexification des processus éditoriaux. En effet, en intégrant des éléments de plus en
plus variés dans les productions éditoriales numériques pour la jeunesse, les éditeurs et
les auteurs se trouvent confrontés à des problématiques économiques et juridiques de plus
en plus diversifiées. Ainsi, quels modèles de rémunération et de gestion des droits utiliser
dans le cas de l’intégration d’une interactivité tactile dans un récit illustré ? Quels seront
les statuts des différents contributeurs dès lors qu’un projet peut rassembler un auteur, un
illustrateur, un animateur ou un développeur ?</p>
</div>
<div class="paragraph">
<p>&#160; Enfin, le dernier aspect à considérer pour l’édition jeunesse est celui des modèles
économiques mis en oeuvre lors du passage au numérique. Ainsi, en investissant les
potentialités du numérique, les éditeurs jeunesse se trouvent confrontés à des choix
stratégiques à la convergence des formats techniques, des circuits de distribution et de
diffusion et des pratiques de lecture. Ces choix, complexes, conditionnent l’ensemble de
la filière. Le choix du format, ePub ou application, est nécessairement associé à un
modèle économique spécifique, et à une diffusion par les acteurs du livre ou par des
plates-formes d’applications (<em>iTunes Store</em> ou <em>Google Play</em>). Les modèles tarifaires et
fiscaux suivent ces logiques et poussent l’édition jeunesse soit vers le champ de l’édition
de livres numériques soit vers celui des applications et du jeu vidéo.</p>
</div>
<div class="paragraph text-justify">
<p>== Les manuels universitaires : nouvelles formes de consultation</p>
</div>
<div class="paragraph">
<p>&#160; Les manuels représentent un versant important de l’édition universitaire. À la
différence des revues savantes destinées principalement à un public de chercheurs ou
d’étudiants en fin de cursus pour accompagner un travail de recherche , les manuels
universitaires visent un public d’étudiants dans un contexte d’apprentissage. Ce marché
est donc tourné vers deux types de clientèles potentielles : étudiants faisant l’acquisition à
titre personnel d’ouvrages qui vont les suivre tout au long d’un enseignement, et
bibliothèques universitaires ou académiques. À cette première clé de segmentation du
marché, il faut en ajouter une seconde qui prend en compte les différences entre les
disciplines. On distingue principalement les sciences humaines et sociales (SHS) des
sciences, techniques et médecine (STM). La confrontation au numérique du secteur des
manuels universitaires conduit à interroger à la fois les pratiques documentaires des
étudiants, leurs comportements d’achat ainsi que leurs approches pédagogiques.</p>
</div>
<div class="paragraph">
<p>&#160; L’ensemble de ces critères dessine une situation de l’édition de manuels
universitaires numériques très contrastée. Par exemple, le modèle de l’édition de manuels
de gestion en premier cycle n’a que peu à voir avec celui des codes juridiques destinés
aux étudiants en droit. Il est toutefois possible d’identifier certains invariants dans
l’évolution des différents modèles. D’une part, les pratiques documentaires des étudiants
sont fortement liées aux pratiques pédagogiques qu’ils rencontrent, notamment la
prescription de lecture. Ces pratiques s’appuient aujourd’hui largement sur des platesformes
pédagogiques dont les fonctionnalités techniques constituent elles aussi un
ensemble de contraintes et permettent la mise à disposition des ressources directement
par les enseignants. D’autre part, les pratiques d’acquisition par les étudiants restent, en
France, largement conditionnées par le marché de l’occasion dans une logique d’achatrevente
et par une forte attente, envers les Bibliothèques universitaires (BU),
d’exhaustivité des ressources documentaires nécessaires au suivi des cursus. Les manuels
numériques doivent donc trouver leur place selon des modalités qui correspondent à ces
pratiques. Ainsi, la définition d’un modèle pour l’édition numérique de manuels
universitaires doit prendre en compte à la fois les spécificités disciplinaires des pratiques
documentaires des étudiants, les contraintes des environnements pédagogiques
universitaires et les attentes des enseignants et des bibliothécaires dans la mise à
disposition des ressources documentaires.</p>
</div>
<div class="paragraph text-justify">
<p>== Les encyclopédies : de nouvelles formes de légitimation</p>
</div>
<div class="paragraph">
<p>&#160; Le secteur des encyclopédies a connu ces dernières décennies deux phases bien
distinctes dans son passage au numérique, séparées par le lancement de Wikipédia qui
marqua un tournant majeur pour ce secteur. La première phase a vu, à partir des années
1990, un « simple » changement de support avec l’arrivée des encyclopédies sur CD puis
DVD. Cette évolution permit aux éditeurs d’exploiter les potentialités du format
numérique comme la puissance de recherche ou l’intégration de contenus multimédias
dans les notices. Cette phase fut accompagnée d’investissements importants des éditeurs
dans des versions enrichies de leur produits, tout en conservant le modèle traditionnel
combinant des auteurs identifiés, parfois connus, rédigeant des notices, et la vente de ces
produits selon une logique classique de vente directe au consommateur. Cette situation a
conduit nombre d’éditeurs d’encyclopédie à renoncer, à partir des années 2000, à leurs
versions imprimées voire dans certains cas à disparaître.</p>
</div>
<div class="paragraph">
<p>&#160; Ce changement de support a été suivi d’une seconde phase, modifiant bien plus en
profondeur ce secteur éditorial, avec l’arrivée de Wikipédia. Cette encyclopédie
collaborative, lancée en 2001, connaît un succès important, qui l’a élevée depuis
plusieurs années au sixième rang des sites les plus visités dans le monde.
Plusieurs facteurs doivent être pris en compte pour analyser ce bouleversement. Le
premier est la croissance d’une pratique de recherche en ligne qui passe de plus en plus
par l’interrogation d’un moteur de recherche, au lieu de la saisie d’une adresse URL dans
un navigateur. De plus, l’intégration de Wikipédia comme une des bases du Knowledge
Graph (base sémantique de connaissances utilisée par Google) donne une visibilité
extrêmement importante à ses notices. En 2016, Wikipédia proposait plus de 1 900 000
notices en français et plus de 46 millions au niveau mondial, dans plus de 280 langues.</p>
</div>
<div class="paragraph">
<p>&#160; Le principal facteur de changement introduit par Wikipédia du point de vue
éditorial réside dans le mécanisme de création et de validation des contenus qu’elle
propose. En effet, la logique contributive à la base de Wikipédia constitue un
renversement du processus de légitimation des contenus encyclopédiques. Celui-ci passe
ainsi d’une validation verticale descendante, apportée par l’éditeur et par la légitimité des
contributeurs, à une logique de validation collective par l’ensemble des contributeurs. Il
découle de cette approche une instabilité systémique des notices proposées, à l’opposé de
la pérennité des encyclopédies traditionnelles inscrites dans une temporalité longue.
Ce n’est donc pas uniquement le changement de support au cours de la première
phase qui explique ce bouleversement du secteur des encyclopédies, mais la conjonction
d’une logique contributive et du développement de parcours de recherche d’information
en ligne dans lesquels les contenus proposés par Wikipédia trouvent très efficacement
leur place. Dans ce nouvel environnement, et en occupant la place prépondérante qu’elle
occupe aujourd’hui, Wikipédia doit déployer des mécanismes de modération et de
validation de plus en plus efficaces pour faire face aux tentatives multiples d’utilisation à
des fins commerciales ou politiques.</p>
</div>
<div class="paragraph">
<p>&#160; Le secteur des encyclopédies présente donc, parfois de manière archétypale,
plusieurs éléments caractéristiques des problématiques éditoriales à l’ère numérique.
D’une part, le changement de support est à considérer à deux niveaux radicalement
différents. Le premier niveau, formel, n’induit pas de modifications structurelles des
modèles en place, les éditeurs étant souvent dans une logique de maintien d’un modèle
économique classique. Le second niveau est plus délicat : en passant d’un support
imprimé à un format numérique, les encyclopédies ont dû appréhender la problématique
nouvelle de la dispersion de leurs contenus et des parcours des utilisateurs en ligne. La
capacité à être visible et repéré dans les pratiques de recherche d’informations est
devenue un point-clé des stratégies numériques des éditeurs d’encyclopédies. D’autre
part, la question de la légitimité éditoriale a constitué un point de bascule incontournable
pour ce secteur. En passant d’une logique de légitimation verticale et descendante à une
approche horizontale et collaborative avec Wikipédia, c’est une des bases du régime
traditionnel de transmission du savoir qui est questionnée.</p>
</div>
<div class="paragraph text-justify">
<p>== L’édition scientifique et savante : légitimation, modèles économiques, usages et nécessité de structuration</p>
</div>
<div class="paragraph">
<p>&#160; L’édition scientifique et savante – à savoir l’ensemble des contenus produits dans le
cadre de la recherche et destinés à un public de spécialistes – compte parmi les champs de
l’édition qui ont été le plus touchés par le développement du numérique. En particulier, le
Web a profondément modifié les pratiques des chercheurs et, par conséquent, le
panorama de l’édition scientifique. On peut identifier deux formes éditoriales principales
dans le domaine scientifique et savant : la revue et la monographie. La revue savante –
dont l’existence remonte au XVIIIe siècle – est le moyen le plus courant pour publier et
faire circuler les résultats de la recherche, spécialement dans le champ des sciences
exactes. L’article scientifique est la forme privilégiée pour partager avec la communauté
scientifique ses recherches et les revues savantes ont été, depuis le XVIIIe siècle, le vecteur
par excellence de ce type de contenu. Les monographies savantes caractérisent plutôt la
production en sciences humaines et sociales – soit des disciplines qui requièrent le
développement d’argumentations plus longues et discursives, ne pouvant pas toujours
être contenues dans la forme courte de l’article. Dans les deux cas, celui de la revue
comme de la monographie, nous pouvons identifier trois défis majeurs posés par les
technologies numériques : en premier lieu, le changement des dispositifs de légitimation,
en deuxième lieu, une transformation des modèles économiques et, en troisième lieu,
l’apparition d’une nouvelle exigence de structuration des contenus qui n’était pas
présente dans le cadre de l’édition imprimée.</p>
</div>
<div class="paragraph">
<p>&#160; L’enjeu de légitimation est crucial pour l’édition scientifique : il faut que le contenu
soit validé scientifiquement. Cette validation se fait par le biais d’un système
d’évaluation par les pairs : chaque contenu – monographie ou article – est donné à lire à
des spécialistes qui donnent leur avis sur la qualité et la rigueur du propos. Ce processus
est normalement pris en charge par la communauté scientifique elle-même (ce sont des
chercheurs qui font les évaluations), mais dans la plupart des cas ce sont des maisons
d’édition qui garantissent la réussite du processus : elles se chargent d’identifier les
évaluateurs, parfois (même si rarement) les payent, suivent le processus pour s’assurer de
sa transparence et de son avancement. C’est grâce à ce travail qu’une maison d’édition
acquiert une plus ou moins bonne réputation et parvient finalement à augmenter la valeur
symbolique des textes qu’elle publie. Un ouvrage paru chez un grand éditeur a un fort
prestige, car son niveau de légitimation sera supérieur. Or le Web met en question ce
modèle, car il produit de nouveaux processus de légitimation : le blog d’un chercheur
peut bénéficier d’une immense visibilité, par exemple, et peut peser dans la légitimation
de contenus. Sans maison d’édition ni même processus d’évaluation, le contenu peut être
cité dans des travaux scientifiques. Un autre aspect qui met en crise les modèles de
légitimation provient des outils de recherche : les moteurs de recherche généralistes sont
de plus en plus utilisés par les chercheurs pour leurs travaux scientifiques [Takševa,
2012] à la place des catalogues des bibliothèques, par exemple. Or cela implique que les
moteurs de recherche – et en particulier Google Search – commencent à obtenir une
fonction de légitimation : ce que l’on trouve sur Google est pertinent et, finalement,
fiable. Les dispositifs de légitimation de l’édition papier sont donc remis en question et
nous sommes appelés à repenser le processus de validation scientifique.</p>
</div>
<div class="paragraph">
<p>&#160; Un deuxième enjeu est lié aux modèles économiques. Le Web a rendu possibles des
formes de publication à très bas coût. Un contenu peut en effet être publié en épargnant
les frais d’impression – qui représentent une part importante des frais éditoriaux. De
façon plus radicale encore, un chercheur peut déposer un texte sur un dépôt ouvert
(comme HAL, ou les dépôts des universités, ou encore des dépôts privés comme
Academia.edu ou ResearchGate) sans même avoir à payer un hébergement, s’il a par
exemple recours aux services d’une plate-forme comme WordPress (la plus utilisée dans
les années 2010 pour ce type d’activité). Dans ce cas, les frais éditoriaux sont réduits à
zéro. Bien évidemment, une publication de ce type ne bénéficie pas (ou pas
complètement) de la médiation de la fonction éditoriale : il n’y a pas de révision par les
pairs, de mise en forme, de diffusion, etc. Mais cette possibilité pose une question
importante à l’édition savante : quel est son rôle et quel est le bon prix que le lecteur peut
être prêt à payer pour profiter du travail éditorial ? La possibilité de la diffusion gratuite
impose une réflexion pour redéfinir à la fois le rôle de l’édition dans le domaine
scientifique et ses coûts. Cette réflexion devient encore plus urgente dans la mesure où
les grands diffuseurs numériques (Elsevier, Springer, Wiley-Blackwell, Taylor &amp;
Francis), plutôt que de corréler la baisse des prix de production à une baisse des prix
d’achat, ont au contraire augmenté ces derniers [<em>cf.</em> par exemple MacColl, 2014]. Les
profits de ces grands diffuseurs sont passés de 665 millions de dollars en 1991 à environ
deux milliards en 2012 [Haustein <em>et al.</em>, 2015].</p>
</div>
<div class="paragraph">
<p>&#160; Le troisième enjeu est lié aux possibilités qu’offre le numérique par rapport au
papier. Les contenus peuvent notamment être structurés grâce à un balisage fin des
informations qui permet ensuite une exploitation automatique – notamment à des fins de
recherche. C’est ce que l’on fait, par exemple, avec des métadonnées – soit des
informations structurées à propos du document : titre, auteur, date de création, etc. C’est
aussi ce que l’on peut faire avec des langages de balisage comme le XML qui permet
d’expliciter le sens et la valeur de certaines parties des documents : par exemple, spécifier
que le mot « Athènes » est le nom français d’une ville qui est la capitale de la Grèce et
qui a certaines coordonnées géographiques. Ajouter ce type d’informations dans un texte
peut être très utile d’un point de vue scientifique, car les données à l’intérieur des
documents sont exploitables : ainsi, un géographe pourrait vouloir repérer tous les textes
qui parlent d’Athènes et de ses alentours et faire une recherche dans les textes à partir des
coordonnées géographiques. Une autre fonctionnalité importante du numérique est de
proposer des versions « augmentées » ou « enrichies » des textes, à savoir des versions
qui contiennent plusieurs types de médias (vidéo, son, image) incorporés dans le texte.
Exploiter ces possibilités demande des compétences – techniques et théoriques – qui ne
faisaient pas partie des compétences éditoriales pour l’édition imprimée. Les acteurs de
l’édition scientifique sont donc appelés à repenser leur fonction et à acquérir de nouvelles
compétences.</p>
</div>
<div class="paragraph">
<p>= La circulation des contenus</p>
</div>
<div class="paragraph">
<p>== Le Web</p>
</div>
<div class="paragraph">
<p>=== L&#8217;importance du Web
Le terme « numérique » a désormais acquis une signification culturelle très large [Doueihi, 2011].
Il n&#8217;est plus utilisé seulement comme un adjectif, mais aussi comme un substantif : le numérique.
Cela signifie que ce terme ne réfère plus seulement à des outils ou à des technologies particulières, mais aussi à un ensemble de transformations sociales, culturelles,  politiques,  économiques  et  anthropologiques  déclenchées  par  le  développement technologique, mais ne dépendant plus exclusivement de ce dernier.
En particulier, lorsque l&#8217;on parle des changements que les technologies numériques ont déterminés dans le champ de l&#8217;édition, on peut faire référence à une multiplicité d&#8217;outils et d&#8217;instruments informatiques, mais on parle aussi de changements institutionnels, sociaux, politiques, etc.
Cependant, on ne peut pas nier que la naissance et la diffusion du Web a eu un impact particulièrement fort sur le monde de la production, de la circulation et de la légitimation des contenus.
Le Web peut être considéré comme la première détermination des caractéristiques de ce que nous appelons le « numérique ».
En particulier, le changement majeur qu&#8217;il a apporté, entraînant ensuite des transformations dans l&#8217;ensemble des autres domaines, est la facilitation de la circulation et de l&#8217;accessibilité des contenus.</p>
</div>
<div class="paragraph">
<p>=== L&#8217;idée du Web : faciliter la circulation
En 1989, Tim Berners-Lee, un informaticien anglais qui travaillait au CERN de Genève, remarque un problème majeur dans la gestion des informations au sein du centre de recherche : les documents ne sont pas assez accessibles et il est très difficile de trouver les informations dont on a besoin, malgré le fait qu&#8217;elles soient potentiellement disponibles.
Le problème identifié par Tim Berners-Lee concerne la circulation de l&#8217;information : les employés du CERN changent souvent et les nouveaux arrivants ont du mal à récupérer les informations dont ils ont besoin, car la seule manière d&#8217;y parvenir est d&#8217;en discuter avec les autres.
Parce que les documents concernant le centre sont en perpétuelle évolution, ils sont difficilement organisables en une structure figée (comme le permettrait un livre imprimé).
C&#8217;est en cherchant la solution à ce problème que l&#8217;informaticien jette les bases, en 1989, du World Wide Web [Berners-Lee, 1989].
L&#8217;idée fondamentale est de trouver une manière de rendre accessibles l&#8217;ensemble des documents en réseau et de les raccorder entre eux via un système de liens.
Concrètement, cela signifie : 1) attribuer à chaque document un nom unique sur tout le réseau (le principe des URL, pour uniform resource locator) ; 2) établir un format universel pour les documents (le HTML, pour hypertext markup language) ; 3) définir un protocole de transmission de ces documents via Internet (le HTTP, pour hypertext transfert protocol).
Ces trois principes permettent la création d&#8217;une nouvelle forme de circulation des contenus : les documents deviennent accessibles à tous ceux qui disposent d&#8217;une connexion Internet.
À partir de 1990, formater un document en HTML et le déposer sur un serveur signifie le rendre public, le « publier ».
La fonction de diffusion des contenus propre à l&#8217;édition a trouvé une nouvelle expression.
Par ailleurs, en raison de la généralisation de la connexion Internet, la diffusion via le Web produit une accessibilité incomparablement plus élevée que celle pouvant être offerte par une maison d&#8217;édition à travers l&#8217;impression.</p>
</div>
<div class="paragraph">
<p>=== Du Web 1.0 au Web 3.0
L&#8217;idée initiale du Web était de mettre à disposition des documents.
Les documents formatés en HTML sont déposés sur des ordinateurs connectés (des serveurs) et deviennent des « pages » accessibles.
Cette première époque du Web, qui va de sa naissance jusqu&#8217;à la fin des années 1990, correspond à ce qu&#8217;on appelle le Web 1.0, ou Web statique.
Le modèle du Web 1.0 est assez proche de celui de l&#8217;édition papier, car il est fondé sur une idée du document comme une ressource stable – exactement comme une page imprimée.
Il y a d&#8217;une part un producteur du document et d&#8217;autre part des lecteurs de ce document : les rôles sont clairement établis.
À partir de la fin des années 1990, le Web commence à s&#8217;orienter vers une nouvelle forme d&#8217;organisation des contenus : alors qu&#8217;il se contentait de mettre en relation une série de documents, il commence à inclure des relations avec des individus.
Le Web est alors fait de documents et de personnes.
C&#8217;est ce que Darcy DiNucci [1999] appelle le Web 2.0 et qu&#8217;on qualifiera par la suite de « Web social ».
Les contenus deviennent dynamiques, car les usagers commencent à pouvoir ajouter eux-mêmes des informations : commentaires,  recommandations,  images,  informations  sur  leurs  profils,  etc.
 TripAdvisor, qui permet aux usagers d&#8217;évaluer restaurants et hôtels, apparaît en 2000.
Wikipédia, l&#8217;encyclopédie participative, est lancée en 2001.
Quant à Facebook et Twitter, ils apparaissent en 2006.
Le rôle des « lecteurs » change radicalement : le lecteur devient contributeur.
En même temps, les contenus perdent leur stabilité : ils sont susceptibles de changer à tout moment.
Ce sont deux transformations majeures par rapport au modèle de l&#8217;édition papier.
Le Web 3.0, ou Web sémantique, ajoute aux relations entre documents et personnes d&#8217;autres relations avec les machines.
Les informations, à partir du moment où elles sont correctement structurées et balisées, peuvent ainsi être utilisées, comprises et réagencées non seulement par les lecteurs, mais aussi par des algorithmes.
Les machines deviennent à leur tour des instances éditoriales, capables d&#8217;organiser les contenus.</p>
</div>
<div class="literalblock">
<div class="content">
<pre> Encadré 1.
L'histoire d'Internet et du Web 1969  Première connexion du ARPANET, réseau physique créé à des fins militaires.
Le réseau fera l'objet d'une présentation officielle en 1972 à l'International Computer Communication Conference, date à laquelle soixante terminaux seront connectés à l'ARPANET.
 1970  Sous la supervision de Stephen Crocker, le Network Working Group crée le premier protocole de transmission de données d'ARPANET, le Network control protocol (NCP).
 1973  Création d'un second protocole, le Transmission control protocol (TCP), par Vinton Cerf et Robert Kahn.
 1982  Devant le nombre croissant d'hôtes se connectant au réseau, Paul Mockapetris crée le Domain Name System (DNS, protocole de correspondance entre IP et nom de domaine).
 1983  Réorganisation du TCP en un nouveau protocole, le Transmission control protocol/Internet protocol (TCP/IP).
ARPANET adopte le protocole TCP le 1er janvier 1983.
 1986  La National scientific foundation (NSF) adopte le protocole TCP/IP et finance la construction d'une dorsale (câble transocéanique) traversant les États-Unis.
 1990  S'inspirant de la notion d'hypertexte de Ted Nelson, Tim Berners-Lee développe le World Wide Web à partir du langage HTML (Hypertext markup language) et du protocole HTTP (Hypertext transfer protocol).
 1992  Développement du navigateur Mosaic, dont hériteront entre autres Netscape et Internet Explorer.
Premier navigateur à afficher directement les images, Mosaic contribue au gain exponentiel de popularité du World Wide Web.
 1994  Tim Berners-Lee fonde le World Wide Web Consortium (W3C), qui assure la standardisation des technologies web et l'évolution de ses différents protocoles.
 1995  L'introduction de Netscape ($NSCP) sur les marchés financiers marque le début de la bulle internet (« dot-com bubble ») et un gain d'intérêt marqué du secteur économique à l'égard du Web.
Après que l'indice NASDAQ a quintuplé en cinq ans, la bulle éclate vers mars 2000.
 1998  Fondation de Google par Larry Page et Sergueï Brin.
 2001  Suite à l'échec relatif de Nupedia, projet d'encyclopédie numérique dont la rédaction et la validation des articles devaient être assurées par des expert, Jimmy Wales et Larry Singer créent Wikipédia avec l'intégration de l'application libre wiki.
 2003  Dale Dougherty propose le terme « Web 2.0 », parfois référencé sous les noms de « Web social » ou « Web participatif », pour désigner une nouvelle phase interactive du Web— interactivité qui était d'emblée envisagée par Berners-Lee à la création du World Wide Web.
 2004  Création de Facebook, alors réservé aux étudiants de Harvard, par Mark Zuckerberg.
Le réseau social sera rendu public en 2006.
 2005  Création de YouTube par Steve Chen, Chad Hurley et Jawed Karim.
Le site web sera racheté par Google en 2006.
 2006  Création de Twitter par Jack Dorsey, Evan Williams, Biz Stone et Noah Glass.</pre>
</div>
</div>
<div class="paragraph">
<p>=== Les limites de la circulation des contenus numériques
L&#8217;expansion du Web a rendue possible une circulation des contenus potentiellement sans limite.
La généralisation de la connexion Internet fait qu&#8217;un document, une fois mis en ligne, devient accessible à des millions de personnes.
Le rêve qui était à la base du projet de Tim Berners-Lee semble s&#8217;être réalisé.
Il faut souligner combien ce rêve s&#8217;insère parfaitement dans la tradition de l&#8217;édition, dont l&#8217;une des principales fonctions est la circulation des contenus.
L&#8217;invention de la presse à caractères mobiles au  XVe  siècle avait déjà permis de multiplier la puissance de diffusion des  contenus.
Les manuscrits, pour circuler, devaient être copiés manuellement : le coût et le temps requis par cette opération en limitaient très fortement la circulation.
Par ailleurs, la circulation était freinée par la difficulté physique de faire voyager le support : un livre arrivait en effet seulement là vers où on le transportait.
À première vue, la circulation sur le Web semble avoir abattu toutes ces frontières : la diffusion des contenus est désormais ubiquitaire, les fichiers peuvent être multipliés et copiés gratuitement, voyager à la vitesse de la lumière, etc.
Ce discours ne résiste pourtant pas totalement à une analyse plus attentive.
On compte au moins quatre limites principales à la circulation des contenus dans les environnements connectés : les barrières techniques et culturelles, les barrières géopolitiques, les barrières linguistiques et, enfin, la différence fondamentale entre accessibilité et visibilité.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Les barrières techniques et culturelles se manifestent principalement à travers les phénomènes de « fracture numérique » (digital divide) et de « fracture culturelle » (cultural divide).
La fracture numérique renvoie à la division entre les zones géographiques ayant accès à une connexion Internet et celles qui n&#8217;en ont pas.
Cette fracture peut se manifester entre différents pays ou zones géographiques (l&#8217;Afrique centrale a par exemple un accès très modeste à la connexion par rapport à l&#8217;Europe occidentale et l&#8217;Amérique du Nord), mais aussi à l&#8217;intérieur d&#8217;un même pays – on sait par exemple que les campagnes sont moins connectées que les villes.
Cela crée une véritable différence sociale et culturelle entre les différentes parties de la population mondiale.</p>
</li>
<li>
<p>À cela s&#8217;ajoutent les barrières géopolitiques, qui correspondent aux politiques de censure et de limitation de l&#8217;accès à l&#8217;information propres à certains pays, tels que la Chine.
Mais même dans des régions connectées et non soumises à la censure, on peut identifier une fracture culturelle : le fossé qui sépare les personnes dotées d&#8217;une culture numérique suffisante pour se servir des médias électroniques et ceux à qui cette culture fait défaut.
Ce problème est réel et particulièrement critique à une époque où plusieurs informations ne sont plus disponibles que sur un support informatique accessible via une connexion.</p>
</li>
<li>
<p>Les barrières linguistiques semblent constituer un fait banal, et pourtant il est important de souligner combien la circulation des contenus continue à s&#8217;opérer à l&#8217;intérieur de communautés linguistiques bien délimitées.
Pour pallier cette difficulté, on assiste à une prédominance de l&#8217;anglais comme lingua franca – avec les conséquences politiques et culturelles que peuvent entraîner une telle suprématie.
C&#8217;est pourquoi la recherche sur les traductions – en particulier sur les traductions automatiques – est devenue fondamentale.
Cependant, les progrès récents réalisés dans le domaine de la traduction automatique sur base statistique par Google Translate, et l&#8217;association de cette approche à des algorithmes fondés sur l&#8217;apprentissage machine, n&#8217;ont fait que laisser dans les mains du géant de la Silicon Valley la gestion de cette limite et la possibilité de la dépasser.
Si bien que la circulation des contenus a tendance à acquérir une structure fortement centralisée, au lieu de favoriser la décentralisation et la dissémination des documents propre au projet initial du Web.</p>
</li>
<li>
<p>Si ces deux premières barrières limitent l&#8217;accessibilité des contenus – en empêchant les usagers d&#8217;avoir, dans le premier cas, un accès matériel et, dans le second cas, un accès cognitif aux ressources –, une dernière barrière reste à prendre en compte : celle de la visibilité.
Car en raison de la quantité considérable de contenus existants (et donc potentiellement accessibles), un nombre très limité de documents est réellement visible.
En effet, pour être visible, un contenu doit être référencé, indexé ou recommandé sur d&#8217;autres plates-formes.
Dans le cas contraire, il est noyé dans la masse d&#8217;informations existantes et demeure invisible.
Mais quels sont les instances qui rendent un contenu visible – et qui, de fait, opèrent véritablement une fonction de diffusion ? Concrètement, il s&#8217;agit des moteurs de recherche – Google en premier lieu – et des réseaux sociaux – comme Twitter et Facebook.
En d&#8217;autres termes, nous sommes encore une fois face au risque d&#8217;une forte centralisation de la fonction éditoriale.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>==  Les librairies et les maisons d&#8217;édition face à la circulation GAFAM</p>
</div>
<div class="paragraph">
<p>=== La constitution d&#8217;un oligopole à franges
La structuration progressive de l&#8217;écosystème numérique a fait émerger une forte concentration des acteurs.
Cette dynamique oligopolistique de concentration des activités numériques autour d&#8217;un nombre restreint de plates-formes est observable dès le début des années 2000 avec pour certaines plates-formes des externalités positives (retombées positives alimentant leur succès) et qui leur permet d&#8217;occuper progressivement des positions fortes sur leurs secteurs d&#8217;activités.
C&#8217;est ainsi que la paysage des moteurs de recherche est passé de quelques dizaines d&#8217;acteurs au début du Web à un marché structuré dans sa quasi-totalité autour de trois acteurs (Google, Yahoo, Bing), hors les exceptions notables de la Chine (Baidu) et de la Russie (Yandex).
Cette évolution est observable dans un grand nombre de secteurs d&#8217;activité avec comme corollaire une concentration des moyens et donc de la capacité à proposer des services innovants et coûteux en terme d&#8217;infrastructures.
Ces nouveaux services permettant de capter de nouveaux usages, ce qui renforce davantage ces positions.
La situation de ces marchés peut ainsi être qualifiée d&#8217;oligopole à franges, c&#8217;est-à-dire un marché structuré principalement autour d&#8217;un nombre réduit d&#8217;acteurs captant la majeure partie du marché et entourés d&#8217;un grand nombre d&#8217;acteurs de taille bien plus réduite se partageant une part très limitée du marché.
Cette tectonique des usages numériques a abouti à l&#8217;émergence d&#8217;acteurs largement dominants regroupés sous l&#8217;acronyme GAFAM pour Google, Apple, Facebook, Amazon et Microsoft.
Ces acteurs occupent aujourd&#8217;hui une place prépondérante sur leur marché.
Google dispose ainsi depuis plusieurs années d&#8217;une part de marché mondiale dans les recherches en ligne supérieure à 75 %.
Son système d&#8217;exploitation Android a capté plus de 80 % de la part de marché des OS mobiles et son système de messagerie Gmail est également largement en tête des services de messagerie.
Le cas d&#8217;Apple est un peu particulier : en effet, depuis l&#8217;arrivée de Samsung et d&#8217;Android sur le marché des smartphones, Apple a perdu sa première position en termes de ventes de smartphones et de part du parc des systèmes d&#8217;exploitation utilisés.
L&#8217;entreprise reste toutefois largement dominante en termes de valeur avec environ 90 % des profits sur le marché des smartphones en 2016.
Pour ces deux acteurs, la part du parc des OS installés induit également la part dans le marché des contenus associés (iTunes Store, App Store ou Google Play).
Ainsi, iBooks représente en 2015 11 % du marché américain du livre numérique en volume et 12 % en valeur.
Sur le marché des réseaux sociaux la position de Facebook est également largement dominante avec, en 2017, plus de 50 % du trafic sur les réseaux sociaux et une base d&#8217;utilisateurs actifs nettement majoritaire avec plus de 1,8 milliard d&#8217;usagers en janvier 2017.
À la même époque, la part de marché de Facebook sur les réseaux sociaux était de 18 %, devant What&#8217;s App à 11 %, également propriété de Facebook.
La place dominante de Microsoft dans le domaine des systèmes d&#8217;exploitation et bien connue, mais pour l&#8217;édition numérique, c&#8217;est Amazon qui est l&#8217;acteur le plus pertinent à observer.
En effet, avec 74 % du volume des ventes de livres numériques aux États-Unis en 2015, Amazon est nettement en position de leader sur ce marché.
Au-delà du livre numérique, la part estimé d&#8217;Amazon dans le commerce en ligne aux États-Unis est estimée à 43 % en 2016.
Les industries culturelles connaissent depuis longtemps ce type de marché, les niveaux de concentration atteints dans le domaine de la musique enregistrée ou de l&#8217;édition en témoignent.
Elles se retrouvent donc confrontées pour leurs activités numériques à une structuration du marché qui questionne leur propre place dans les filières.
L&#8217;édition académique a également été touchée par ces changements de modèles.
On peut distinguer deux périodes Au cours de la première période la logique de platesformes et de structuration oligopolistique du marché joua à plein, avec l&#8217;émergence d&#8217;acteurs atteignant des positions très fortes et rassemblant des volumes de ressources éditoriales de plus en plus importants.
Cette situation s&#8217;accompagna d&#8217;une augmentation des budgets d&#8217;acquisition des bibliothèques (principale clientèle de ce type de documentation) pour les ressources numériques.
Une seconde période s&#8217;est ensuite ouverte, en partie en réaction à l&#8217;augmentation des tarifs, avec le développement d&#8217;un modèle tourné vers un accès ouvert aux productions éditoriales scientifiques, s&#8217;appuyant sur les outils du Web permettant cette désintermédiation.
Pour l&#8217;ensemble des secteurs éditoriaux, les conséquences principales de cette situation doivent être analysées de façon précise tant elle conditionne les modèles susceptibles d&#8217;être déployés dans les différents filières et notamment dans celle de l&#8217;édition.</p>
</div>
<div class="paragraph">
<p>=== Poids des modèles, évolution et comparaison internationale
La première conséquence de l&#8217;arrivée des GAFAM dans le monde du livre est une évolution forte des modèles de circulation.
Nous en détaillerons trois : le modèle éditorial, le modèle de la publication Web et celui de l&#8217;édition académique.</p>
</div>
<div class="paragraph">
<p><strong>Le modèle éditorial.</strong>
L&#8217;activité éditoriale s&#8217;est structurée depuis longtemps dans un modèle d&#8217;affaires et d&#8217;organisation de filière stabilisé dans lequel les éditeurs occupent une place centrale [Chartron, 2016].
Du fait de sa position très en amont dans la filière, l&#8217;éditeur dispose de plusieurs leviers de pilotage.
Ainsi, en sélectionnant les contenus qui seront édités, il joue un rôle clé dans l&#8217;alimentation du circuit de distribution.
Il maîtrise également, notamment en France, les prix de vente et la répartition de la remontée des recettes du fait de la présence dans les principaux groupes d&#8217;édition d&#8217;acteurs en charge de la distribution.
Ce dernier aspect est un élément central dans la construction des groupes d&#8217;édition (il a d&#8217;ailleurs joué un rôle important dans les mouvements de concentration qu&#8217;a connu l&#8217;édition française depuis les années 1980).
Cette intégration des plates-formes de distribution est en effet un facteur important puisqu&#8217;elle permet de fixer les conditions de vente et de remise qui influent directement sur les marges des libraires et des éditeurs, surtout sur un marché à prix unique comme en France.
Ce modèle est largement remis en question avec l&#8217;arrivée des GAFAM et le passage à l&#8217;édition numérique.
C&#8217;est l&#8217;arrivée d&#8217;Amazon et la croissance très rapide de sa part de marché dans la vente de livres en ligne qui a tout d&#8217;abord modifié les rapports de forces.
En se positionnant dès sa création sur le créneau de la vente en ligne de livres imprimés, Amazon a développé l&#8217;offre de titres de loin la plus importante aujourd&#8217;hui et des capacités logistiques de haut niveau.
Au cours de son développement mondial, l&#8217;entreprise a également constitué une base de clientèle de plusieurs centaines de millions de personnes.
En atteignant ce poids sur le marché du livre physique, Amazon a inversé en partie le rapport de forces entre l&#8217;amont de la filière (l&#8217;édition) et l&#8217;aval (la distribution).
Cette inversion est très importante, car elle ne se limite pas aux ouvrages sur support papier mais touche également le livre numérique.
Sur ce marché, Amazon occupe une place de leader, ayant largement participé au développement du marché aux États-Unis avec la commercialisation d&#8217;une liseuse, la Kindle.
Lancée en 2007 aux ÉtatsUnis et en 2009 en France, elle est régulièrement mise à jour et constitue pour Amazon un vecteur central dans la construction de son écosystème du livre numérique.
 Avec le développement du marché du livre numérique, les GAFAM bouleversent encore un peu plus le modèle éditorial.
L&#8217;ensemble de ces firmes ont investi d&#8217;une façon ou d&#8217;une autre dans le marché du livre numérique, à des niveaux et avec des stratégies différentes.
Google a, dès 2004, lancé Google Print (qui deviendra Google Books ou Google Livres), programme de numérisation de masse d&#8217;ouvrages essentiellement issus de bibliothèques partenaires.
S&#8217;il a donné lieu à plusieurs actions en justice, Google Livre constitue aujourd&#8217;hui un corpus de quelque 25 millions d&#8217;ouvrages.
Apple a ouvert son iBooks Store simultanément au lancement de l&#8217;iPad, en 2010.
Comme l&#8217;entreprise l&#8217;avait fait pour la musique, les applications ou les jeux, l&#8217;objectif est de proposer des contenus pouvant alimenter un dispositif numérique et ainsi encourager son achat.
Microsoft, qui s&#8217;était associé en 2012 à Barnes &amp; Nobles et à sa liseuse Nook avant de mettre fin au partenariat en 2014, envisageait à nouveau en 2017 de proposer des livres numériques à la vente via une librairie intégrée à son système d&#8217;exploitation.
Enfin, Amazon a encore fait évoluer son offre numérique en lançant en 2014 Kindle Unlimited, qui propose par abonnement mensuel l&#8217;accès, sous conditions, à plusieurs centaines de milliers de livres numériques.
Ainsi, à l&#8217;exception de Facebook, les principaux acteurs du numérique et les plus importantes plates-formes en ligne ont tous une activité en lien avec le livre numérique.
Pourtant, celui-ci occupe une place particulière dans leurs modèles d&#8217;affaires.
En effet, pour l&#8217;ensemble de ces exemples, le livre numérique ne constitue pas une source directe de valeur ajoutée.
Il joue un rôle de « produit d&#8217;appel » dont la valeur sera réalisée au travers d&#8217;autres services ou offres.
Dans le cas d&#8217;Apple, c&#8217;est la vente de matériels qui constitue, de loin, la principale source de revenus.
Pour Google, la logique est différente : en proposant un corpus de cette taille, c&#8217;est une captation de l&#8217;attention qui est visée et donc sa valorisation via les mécanismes publicitaires qui produisent l&#8217;énorme majorité des bénéfices de Google (il convient d&#8217;ajouter que les ouvrages numérisés constituent également un outil précieux dans le développement des services de traduction automatique sur lesquels travaille la firme depuis de nombreuses années).
La part du livre numérique dans l&#8217;activité d&#8217;Amazon est compliquée à mesurer, mais il est probable qu&#8217;il joue également un rôle dans l&#8217;accroissement de sa base de clients, notamment à travers son service premium.
Cette analyse du rapport à l&#8217;édition des GAFAM souligne bien les enjeux actuels de l&#8217;édition numérique, s&#8217;agissant des rapports de forces entre éditeurs et plates-formes, mais aussi de la place du livre, en l&#8217;occurrence numérique, dans la stratégie de ces dernières.</p>
</div>
<div class="paragraph">
<p><strong>Le modèle de la publication Web.</strong>
Nous proposons de rassembler sous cette terminologie un ensemble de modèles d&#8217;affaires qui sous-tendent en partie l&#8217;activité de publication numérique prise dans une acception large, couvrant aussi bien des textes que des contenus audiovisuels ou multimédia.
Dans l&#8217;ensemble de ces modèles le principe est le même, à savoir une dissociation nette entre les circuits de distribution/diffusion et les circuits de financement.
Dans les modèles traditionnels de publication, les mécanismes de remontée des recettes sont bien connus.
Soit il s&#8217;agit d&#8217;une logique de paiement à l&#8217;acte qui amorce une remontée financière tout au long de la filière (par exemple la vente d&#8217;un ouvrage correspond à la perception par le libraire, le distributeur, l&#8217;éditeur, l&#8217;auteur… d&#8217;un montant calculé comme une part du prix de vente).
Soit, et de façon non exclusive, il s&#8217;agit d&#8217;une logique de fructification de l&#8217;attention suscitée par le produit (le fameux « temps de cerveau disponible »).
Dans cette seconde logique, des annonceurs achètent au diffuseur une partie de cette attention via un support publicitaire dont le prix est calculé en fonction de l&#8217;audience captée par le produit (c&#8217;est le cas par exemple lorsqu&#8217;un annonceur achète un espace publicitaire sur une chaîne de télévision).
Ce faisant, ils alimentent également une remontée des recettes qui sera partagée entre les différents acteurs de la filière.
Ce type de marché, dit biface, combine une source de revenus issue de ventes à l&#8217;utilisateur final et un financement publicitaire.
Sur le Web, les modèles utilisés pour la publication se sont largement construits sur un seul versant, celui de la publicité.
Cette approche a longtemps été la principale solution de financement pour les publications en ligne : la vidéo est dans cette logique d&#8217;accès gratuit avec comme contrepartie des messages publicitaires ajoutés au début ou au cours de la vidéo.
La presse en ligne, comme la musique, ont également longtemps exploité ce modèle.
Pour le secteur de l&#8217;édition, cette approche ne s&#8217;est jamais réellement déployée, le poids symbolique du livre freinant sûrement la juxtaposition de messages publicitaires et de contenus éditoriaux.
De ce fait, la publication sur le Web utilise plusieurs mécanismes de financement : la publicité, le financement amont (subvention par exemple), le freemium (un accès gratuit au contenu mais proposant des formats ou services complémentaires payants) ou encore le financement participatif.</p>
</div>
<div class="paragraph">
<p><strong>Le modèle académique.</strong>
L&#8217;édition académique est constituée de deux marchés différents, celui des ouvrages ou manuels de formation et celui des revues de recherche.
Historiquement, les auteurs d&#8217;articles parus dans les revues académiques ont très tôt investi la publication en ligne avec la création d&#8217;archives ouvertes dédiées à la publication et au partage d&#8217;articles.
Les acteurs commerciaux ont également proposé très tôt des offres numériques avec un accès à distance aux revues numériques.
À l&#8217;occasion de ce changement de support, les éditeurs scientifiques ont transposé leur modèle classique, celui de l&#8217;abonnement, en proposant aux institutions abonnées un accès en ligne à des « bouquets » de revues numériques.
Ces abonnements étaient onéreux mais, ramenés au nombre de titres, présentaient encore un intérêt pour les bibliothèques, même si cela s&#8217;est amenuisé avec la hausse importante des tarifs imposée par les grands éditeurs scientifiques.
Le modèle de l&#8217;abonnement utilisé pour les publications académiques (revues ou ouvrages), papier d&#8217;abord et numérique ensuite, demeure cependant particulièrement intéressant.
Du point de vue économique, l&#8217;abonnement, ou plus précisément la licence d&#8217;accès, correspond à un modèle dans lequel le tarif est partiellement décorrelé de l&#8217;usage.
En effet, le paiement par une institution académique ou une bibliothèque universitaire d&#8217;une licence pour des publications scientifiques ouvre l&#8217;accès à l&#8217;ensemble des usagers de cette institution.
Quels que soient les accès effectifs à ces ressources, le montant restera globalement forfaitaire.
Ce type d&#8217;offre présente deux caractéristiques : d&#8217;une part, le paiement d&#8217;un abonnement forfaitaire pour accéder à une offre de contenus ; et, d&#8217;autre part une collection de titres ou de contenus extrêmement importante.
Ce modèle de licences s&#8217;est également développé dans les secteurs de la musique avec Deezer en 2007 et Spotify en 2008, dans le secteur du jeu vidéo dès 2003 avec Steam ou dans celui de l&#8217;audiovisuel avec Netflix en 2010.
Il est également présent dans l&#8217;édition grand public via l&#8217;offre Kindle Unlimited d&#8217;Amazon depuis 2014.
Reconnaissance des formats et marché Le numérique, questionne également les modèles éditoriaux traditionnels du point de vue des formats – nous désignons ici par « format » des formes éditoriales et non des formats informatiques de fichiers.
Avec l&#8217;arrivée des nouveaux formats numériques, les libraires et éditeurs sont amenés à revoir leur pratiques dans la construction de gammes de produits, et ce à deux niveaux.
À un premier niveau, on assiste à un certain nivellement des livres numériques vers un type unique de format(age) éditorial, indépendamment du fichier, qui ne propose pas au lecteur d&#8217;expériences de lecture différenciées.
Il n&#8217;existe pas, pour le livre numérique, de logique de gamme ou de formats comme le support papier l&#8217;autorise : les éditions « grand format », « poche », « limitée », « luxe »… n&#8217;y ont en effet pas de sens, la perception qu&#8217;a le lecteur d&#8217;un livre numérique étant principalement conditionnée par le dispositif de lecture dont il dispose.
Il n&#8217;est donc pas possible de faire varier la forme de l&#8217;ouvrage numérique pour en proposer différentes déclinaisons.
Un des leviers disponible pour repenser cette construction de gamme est de segmenter l&#8217;offre selon le niveau de services accompagnant l&#8217;ouvrage numérique.
Ces services associés peuvent notamment consister en l&#8217;ajout de contenus illustratifs, d&#8217;approfondissement, etc.
Cela revient en réalité à produire deux versions (ou éditions) d&#8217;un ouvrage : une version complète et une version partielle proposant uniquement le texte, sans les enrichissements.
Les services complémentaires peuvent également être liés à la portabilité – c&#8217;est-à-dire la possibilité de lire le livre numérique sur plusieurs supports différents (smartphone, tablette, ordinateur, livre lu…) – ou à l&#8217;accessibilité de l&#8217;ouvrage numérique.
Il s&#8217;agit d&#8217;une approche déjà utilisée dans certains domaines avec au départ un accès à une version en ligne de l&#8217;ouvrage ou du texte, et, en montant en gamme, l&#8217;accès à d&#8217;autres versions, souvent autonomes, sous forme de fichiers à utiliser sur tablette ou liseuse.
Cette montée en gamme peut également correspondre à des accès simultanés ou multiples à l&#8217;ouvrage, permettant par exemple une lecture suivie sur plusieurs dispositifs.
Cette difficulté de segmentation du marché pour l&#8217;édition numérique est liée, à un deuxième niveau, à la question de la fixation des prix de vente.
Depuis les débuts de l&#8217;édition numérique, la question du prix du livre numérique s&#8217;est posée, du point de vue économique et législatif.
Du point de vue économique la difficulté consiste pour l&#8217;éditeur à définir un prix de vente qui intègre la nouvelle structure de coûts (plus de coûts d&#8217;impression mais des coûts de développement), le nouveau circuit de distribution, la disparition des gammes (moyenne entre grand format et poche) et l&#8217;acceptation par les acheteurs.
Cette équation est d&#8217;autant plus complexe à résoudre que certaines platesformes de distribution comme Amazon encouragent une uniformité des prix (9,99 $ pour un ouvrage), comme Apple l&#8217;a fait pour la musique (0,99 $ pour un titre).
Si la question de la fixation des prix n&#8217;est pas complètement nouvelle pour les éditeurs, la souplesse potentielle dans la variation des niveaux de prix est par contre une vraie nouveauté.
 Contrairement à l&#8217;édition papier, l&#8217;édition numérique dispose d&#8217;un nouveau levier marketing ou commercial via la possibilité de faire varier le prix du livre numérique tout au long du cycle de vie.
Cette possibilité (proposer un prix de lancement, remonter ensuite au tarif normal ou proposer des variations liées à l&#8217;actualité par exemple) est déjà largement utilisée sur le marché des applications mobiles mais nécessite, de la part des éditeurs, des compétences nouvelles dans l&#8217;analyse en temps réel des volumes de ventes déclencher ou mesurer les effets des fluctuations tarifaires.</p>
</div>
<div class="paragraph">
<p>== Les bibliothécaires face à la « grande bibliothèque numérique »
L&#8217;édition et le livre sont au cœur de l&#8217;idée de bibliothèque.
Les ouvrages sur support papier en ont conditionné le modèle même : ses espaces physiques, construits autour des rayonnages, l&#8217;identité de ses professionnels ou encore son inscription dans la cité.
Les développements du Web et du livre numérique amènent les bibliothèques vers un renouvellement de leur rapport à leur objet, de leurs pratiques et de leur définition en tant qu&#8217;institution.</p>
</div>
<div class="paragraph">
<p>=== Le Web comme grande bibliothèque
Depuis les débuts du Web, le parallèle avec les bibliothèques n&#8217;a cessé d&#8217;être développé et analysé.
Il s&#8217;appuie sur deux caractéristiques communes au Web et aux bibliothèques : la facilité d&#8217;accès à un coût très faible et la mise à disposition de vastes collections de ressources accessibles à tous.
Ce rapprochement entre Web et bibliothèques se fonde également sur un aspect que nous avons déjà évoqué précédemment, le paiement forfaitaire décorrelé de l&#8217;usage effectif des ressources proposées.
Ce modèle de licence ou d&#8217;abonnement, qui se diffuse aujourd&#8217;hui à un nombre croissant de secteurs d&#8217;activité en ligne, est historiquement celui de la bibliothèque.
L&#8217;analogie entre les modèles va même plus loin.
Dans les deux cas, s&#8217;il est proposé à l&#8217;utilisateur d&#8217;accéder gratuitement à une vaste collection de ressources, les services supplémentaires – comme le prêt, la suppression des publicités ou l&#8217;accès hors ligne (pour la musique par exemple) – seront payants ou nécessiteront une démarche d&#8217;identification.
Cette évolution importante des possibilités d&#8217;accès à des ressources, via la numérisation et ces nouveaux modèles, fait évoluer le rôle des bibliothèques et questionne la spécificité de leur modèle.
Le rapprochement entre les modèles de la bibliothèque et des industries culturelles est renforcé par la numérisation progressive des contenus proposés par les deux types d&#8217;acteurs.
Cette numérisation conduit à des pratiques de recherche, de consultation et d&#8217;accès qui passent par les mêmes dispositifs numériques.
La consultation en ligne des ressources proposées par la bibliothèque se fait via le même terminal que pour consulter librement des ressources publiées en ligne ou commercialisée.
Cette proximité entre les différents fournisseurs, rassemblés du point de vue de l&#8217;usager sur le même dispositif, renforce la mise en concurrence de l&#8217;ensemble des acteurs, bibliothèques comprises, dans l&#8217;offre de contenus numériques.
Cette mise en concurrence ne se limite pas à des questions de coûts mais couvre l&#8217;ensemble des aspects de l&#8217;expérience utilisateur : qualité des interfaces, facilité d&#8217;usage ou adéquation avec son propre écosystème numérique.
En effet, avec le passage au support numérique, l&#8217;ensemble des acteurs du livre entre dans un nouvel écosystème, en ligne, dans lequel chacun doit réinventer ses rôles, modèles et valeurs ajoutées.
Les bibliothèques n&#8217;échappent pas à ces enjeux.
Ainsi, plus encore que les médias qui le précédèrent dans le développement numérique, le livre joue un rôle majeur dans l&#8217;évolution des bibliothèques.
Cette évolution est observée et mesurée depuis plusieurs années pour les bibliothèques universitaires et académiques, en raison principalement du développement des offres numériques des revues spécialisées.
Le développement des pratiques et la croissance de l&#8217;offre de livres numériques entraînent les bibliothèques de lecture publique vers les mêmes questionnements.
Car il ne s&#8217;agit pas pour le livre numérique d&#8217;un « simple » changement de support comme le passage de la VHS au DVD ou du disque vinyle au CD.
Il s&#8217;agit d&#8217;un changement à la fois de modèle (l&#8217;acquisition, la sélection, l&#8217;indexation, la mise à disposition ou la conservation changent radicalement) mais aussi de place dans les écosystèmes de l&#8217;édition et plus largement dans les pratiques des usagers.
En intégrant le livre numérique à ses collections la bibliothèque se positionne dans une offre éditoriale numérique bien plus vaste, accessible en ligne.
Si ce rôle de repérage et de signalement de ressources en ligne n&#8217;est pas nouveau ou lié uniquement au livre numérique, il est toutefois aujourd&#8217;hui un enjeu fort de la visibilité des bibliothèques.
En effet, depuis les premiers temps du Web les modalités d&#8217;accès aux ressources publiées en ligne ont largement évolué.
D&#8217;une identification des différents acteurs, publics ou privés, via leur site web les usages sont passés à un accès direct aux ressources via les moteurs de recherche.
La visibilité institutionnelle des bibliothèques au travers de leur site web n&#8217;est donc plus suffisante, celui-ci ne constituant plus un point de passage obligé pour accéder aux collections.
L&#8217;enjeu, et les bibliothèques ont fortement investi ce champ, est donc la visibilité, la « trouvabilité » de l&#8217;offre des bibliothèques sur le chemin de navigation des internautes.
Cette nouvelle donne implique deux enjeux différents.
D&#8217;une part il est primordial pour les professionnels des bibliothèques de maîtriser les techniques d&#8217;intégration des ressources des bibliothèques dans les résultats des moteurs de recherche.
Cette intégration suppose une ouverture des catalogues au Web pour permettre aux moteurs de recherche d&#8217;indexer efficacement, titre à titre, l&#8217;offre des bibliothèques.
D&#8217;autre part, la diffusion des métadonnées des catalogues aux moteurs de recherche pose la double question de la visibilité des bibliothèques en tant qu&#8217;institution et de leur ancrage territorial.</p>
</div>
<div class="paragraph">
<p>=== Bibliothèques et algorithmes
Avec le passage sur support numérique de l&#8217;offre documentaire des bibliothèques une autre dimension du rôle des bibliothèques a également été modifiée.
En effet, il appartenait jusqu&#8217;à présent aux bibliothèques d&#8217;établir l&#8217;ensemble des relations entre les documents de leur fonds.
Cette mise en réseau s&#8217;incarne toujours aujourd&#8217;hui dans les pratiques de catalogage et de cotation des documents.
En attribuant à un document une côte et donc un emplacement, physique ou virtuel, dans une collection, les bibliothécaires construisent un maillage sémantique qui relie les éléments de la collection sur la base des métadonnées extraites ou produites autour des documents.
Il s&#8217;agit d&#8217;une expertise professionnelle au cœur des métiers des bibliothèques qui consiste à attribuer, via une notice par exemple, un ensemble d&#8217;attributs, libres ou prédéfinis.
Ces attributs, côte, mots-clés, thématiques… permettent de rassembler au sein d&#8217;une organisation tangible (les étagères d&#8217;une bibliothèque) ou virtuelle (la page de résultats du catalogue) des ressources documentaires.
Cette construction sémantique, à l&#8217;échelle de la collection ou d&#8217;un sous-ensemble, constitue une valeur ajoutée unique des bibliothèques.
Elle est établie sur un temps long et s&#8217;appuie sur des outils et des méthodes spécifiques et éprouvées.
Cet aspect du travail des bibliothèques ne leur est pas exclusif, il se retrouve dans de très nombreux secteurs d&#8217;activité, de la librairie au supermarché.
La particularité réside pour les bibliothèques dans deux aspects : le des documents traités, et le caractère intellectuel des ses traitements.
En effet, la production de métadonnées nécessite en bibliothèques une connaissance des domaines traités, une interprétation éventuelle des informations disponibles (titre, éditeur…), etc.
Cette expertise limitait jusqu&#8217;à présent l&#8217;automatisation d&#8217;une partie de ces tâches et a conduit les bibliothécaires à développer des pratiques renforcées de mutualisation et de partage.
Le glissement vers le numérique des documents peut être appréhendé au travers de trois aspects particuliers de ce type d&#8217;activité des bibliothèques.
Tout d&#8217;abord, comme nous l&#8217;avons évoqué, les bibliothécaires, en tant que professionnels de l&#8217;information, disposent de compétences élevées dans la production de métadonnées, la manipulation de données structurées et l&#8217;utilisation d&#8217;outils puissants d&#8217;interrogation de corpus.
La difficulté est de positionner la bibliothèque, soit comme point d&#8217;entrée vers des collections provenant en partie du Web (sur lesquelles le niveau de stabilité et de structuration des données n&#8217;est pas garanti) ou vers des ressources provenant de bases connues et identifiées (archives ouvertes par exemple) ; soit comme fournisseur de ressources, par exemple de données bibliographiques, qui seront utilisées et reprises par d&#8217;autres points d&#8217;accès comme les moteurs de recherche.
Dans ce dernier cas, c&#8217;est la perception de la bibliothèque par les usagers qui posera question.
Ensuite, les documents numériques autorisent une forme nouvelle de traitement algorithmique qui s&#8217;appuie sur les documents directement et non sur les métadonnées.
C&#8217;est déjà le cas pour les textes, aisément manipulables par des outils informatiques, mais cette capacité de traitement automatisé et algorithmique se déploie également pour les autres formats comme la vidéo ou la musique.
Cette capacité offerte par le numérique de confier la production des métadonnées (des index dans la plupart des cas) à un traitement logiciel fait glisser l&#8217;expertise du traitement documentaire des professionnels de l&#8217;information aux spécialistes des outils informatiques.
Cette approche n&#8217;est évidemment pas exempte de difficultés et de limites.
Le bruit, par exemple, dans les résultats d&#8217;une recherche, c&#8217;est à dire la propension du système à proposer des résultats non pertinents par rapport à une requête, est important (il suffit d&#8217;observer le nombre de résultats d&#8217;une requête sur un moteur de recherche) mais la masse de documents traités et la capacité de l&#8217;algorithme à ordonner les résultats selon des critères dits de « pertinence » suffisent apparemment à répondre aux attentes des usagers.
Il s&#8217;agit d&#8217;une évolution importante car elle place l&#8217;algorithme comme principal outil de traitement et d&#8217;interrogation des corpus documentaires, sans avoir recours à un traitement et une production humaine de métadonnées.
Enfin, la place centrale prise par les algorithmes dans l&#8217;indexation et l&#8217;interrogation des corpus documentaires a conduit les bibliothèques à déployer deux approches.
D&#8217;une part une évolution des interfaces d&#8217;interrogation des catalogues qui, en se rapprochant des moteurs de recherche, amène à masquer la complexité et la finesse des métadonnées exploitables.
Cette approche pose en creux la question du retour sur investissement du travail conséquent de production manuelle de métadonnées.
Cette interrogation est d&#8217;autant plus forte qu&#8217;on assiste à une forme d&#8217;externalisation des activités d&#8217;indexation (lorsque les ressources sont acquises avec des métadonnées déjà établies) et des outils d&#8217;interrogation (comme avec les outils de découverte, qui permettent une interrogation unique de multiples réservoirs de contenus).
D&#8217;autre part, les bibliothèques ont travaillé à la diffusion et à l&#8217;intégration de leurs propres métadonnées dans les index des moteurs de recherche.
Ce faisant elles se placent ainsi dans le champ de vision des usagers lorsqu&#8217;ils utilisent un moteur de recherche pour leurs recherches documentaires.
Le dernier aspect de la médiation algorithmique à l&#8217;œuvre sur le Web est l&#8217;exploitation par les algorithmes des données d&#8217;usage issues des pratiques des internautes.
Cela correspond à des logiques de recommandation bien connues sur les réseaux sociaux mais également utilisées par des moteurs de recherche.
Il s&#8217;agit d&#8217;exploiter les données d&#8217;usage provenant des navigations de l&#8217;internaute, des liens qu&#8217;il suit ou encore de ses achats.
À partir de ces données, il devient possible de construire un maillage de proximité entre des pages web ou des produits comme nous l&#8217;évoquions pour les bibliothèques.
À la différence principale que cette proximité est construite de façon automatisée et individualisée.
Dans cette confrontation des bibliothèques aux algorithmes d&#8217;indexation, d&#8217;interrogation et de recommandation celles-ci ne sont pas dépourvues d&#8217;atouts.
Leur statut d&#8217;acteur public leur permet en effet de proposer une véritable transparence concernant les algorithmes qu&#8217;utilisent leurs outils.
Par rapport aux moteurs de recherche, cette capacité de rendre public le fonctionnement de leurs outils représente une réelle valeur ajoutée.
Elle permet de mettre en avant le caractère neutre du traitement documentaire et de l&#8217;accès proposés en bibliothèques, cette neutralité et cette ouverture pouvant aujourd&#8217;hui rencontrer un écho favorable auprès des usagers.
Cette neutralité se retrouve également dans le fait que l&#8217;indexation, et donc les proximités entre les ouvrages, sont établies de manière stable, en amont et indépendamment des usages.
Cela induit une médiation et une recommandation basée uniquement sur une logique sémantique et non sur les intérêts commerciaux de la mise en avant d&#8217;un document ou d&#8217;un site Web en particulier.
 Ce déploiement des potentialités d&#8217;indexation algorithmiques offertes par le format numérique ouvrent, entre autres, deux champs de réflexion pour les bibliothèques.
Le premier champ concerne l&#8217;exploitation des données d&#8217;usage pour des services de recommandation ou de médiation.
Cette question est complexe pour les bibliothèques.
Elle s&#8217;inscrit dans une tension délicate entre d&#8217;une part la garantie d&#8217;une forme relative d&#8217;anonymat, c&#8217;est-à-dire la garantie que les données d&#8217;usage ne seront exploitées que pour permettre le fonctionnement  normal de l&#8217;établissement (prêts, retours,  l&#8217;identification auprès des fournisseurs de ressources en ligne…) et d&#8217;autre part la mise en œuvre de services exploitant ces données, de la forme « ce qui ont emprunté ça ont aussi emprunté ça ».
Cette réflexion doit prendre en compte plusieurs aspects : le cadre juridique qui définit les conditions de conservation et d&#8217;utilisation des données personnelles en bibliothèque, l&#8217;attente des usagers de retrouver dans leur bibliothèque des services similaires à ceux qu&#8217;ils utilisent ailleurs sur le Web ou encore l&#8217;importance qu&#8217;ils accordent à la protection de leurs données personnelles.
Le deuxième champ est celui de l&#8217;exploitation des potentialités offertes par la dématérialisation des collections pour proposer des interfaces d&#8217;exploration innovantes.
Avec le livre papier, les contraintes physiques liées au rangement de volumes dans l&#8217;espace réel amènent les bibliothèques à choisir des modalités d&#8217;organisation qui privilégient la proximité.
Ainsi, dans la plupart des bibliothèques, la disposition des espaces et des documents donne aux usagers une indication sur l&#8217;organisation, généralement thématique, des collections.
Les classifications utilisées en bibliothèques, comme la classification de Dewey, définissent l&#8217;arborescence de ces thématiques.
Cette classification choisie par la bibliothèque est la seule proposée aux usagers.
Celle-ci placera tel ouvrage à côté de tel autre, imposant ainsi une modalité unique de découverte et d&#8217;exploration.
L&#8217;édition numérique ouvre de nouvelles possibilités dans ce domaine.
Affranchie des contraintes physiques, les collections peuvent être structurées autour d&#8217;une multitude d&#8217;axes différents et ainsi être abordées selon des critères très différents (par exemple le nombre d&#8217;emprunts, la date d&#8217;acquisition, la longueur ou la mention d&#8217;une personne ou d&#8217;un lieu).
Ces possibilités de structuration autour de critères différents, choisis potentiellement par l&#8217;utilisateur, créent de nouvelles proximités entre les ouvrages.
Elles permettent une exploration et une sérendipité bien plus dynamiques que sur papier, à condition que les interfaces proposées le permettent.
Les listes proposées par les catalogues, bien que pouvant souvent être triées selon de multiples critères, restent encore trop limitées pour permettre la même facilité de découverte que la déambulation dans les espaces physiques des bibliothèques.</p>
</div>
<div class="paragraph">
<p>=== Communautés et circulation
En intégrant des produits éditoriaux numériques dans leurs collections, les bibliothèques déploient progressivement une offre documentaire adressée non plus à un territoire mais à une communauté d&#8217;usagers.
Cette communauté, caractérisée jusqu&#8217;alors par une proximité géographique avec les établissements peut s&#8217;affranchir des contraintes de déplacement pour se rassembler autour d&#8217;une thématique ou d&#8217;un corpus particulier (un exemple de ces communautés est celle rassemblée autour des ressources proposées dans Gallica par la BNF).
En utilisant les réseaux sociaux ou leur propre site web, les bibliothèques construisent des communautés d&#8217;usagers en ligne, qui ne s&#8217;inscrivent plus dans un territoire défini.
Plus largement, l&#8217;édition numérique amène les bibliothèques à interroger le maillage territorial existant à l&#8217;aune d&#8217;une offre dématérialisée accessible de n&#8217;importe où.
Dans certains établissements des aspects administratifs ou réglementaires limitent l&#8217;inscription des usagers à ceux issus d&#8217;une zone géographique précise, ce n&#8217;est pas le cas partout et cela n&#8217;apporte pas d&#8217;éléments de réponse quant à la pertinence de ce type de maillage d&#8217;un territoire virtuel.
En s&#8217;affranchissant des contraintes logistiques et matérielles d&#8217;accès physique aux ouvrages, l&#8217;édition numérique questionne le sens des communautés d&#8217;usagers construites autour des bibliothèques.
Elle ouvre des opportunités réelles de toucher un public plus large, disséminé sur des territoires plus vastes, parfois éloigné ou empêché dans son accès à la bibliothèque comme bâtiment.
Ces nouvelles communautés d&#8217;usagers amènent une nouvelle conception du rôle de la bibliothèque comme espace public, numérique, et comme espace de sociabilisation.
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 0788ddd98a62ac76eb427cee793d77aa8b8515c9</p>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<div id="footer">
<div id="footer-text">
Version v1<br>
Last updated 2018-12-07 10:57:56 UTC
</div>
</div>
</body>
</html>